
# PFI

## Agile software development

In software development, agile (sometimes written Agile) practices involve discovering requirements and developing solutions through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, early delivery, and continual improvement, and it encourages flexible responses to change.
It was popularized by the Manifesto for Agile Software Development. The values and principles espoused in this manifesto were derived from and underpin a broad range of software development frameworks, including Scrum and Kanban.
While there is much anecdotal evidence that adopting agile practices and values improves the agility of software professionals, teams and organizations, the empirical evidence is mixed and hard to find.

### Méthode agile

Vous pouvez améliorer la vérifiabilité en associant ces informations à des références à l'aide d'appels de notes.
En ingénierie logicielle, les pratiques agiles mettent en avant la collaboration entre des équipes auto-organisées et pluridisciplinaires et leurs clients. Elles s'appuient sur l'utilisation d'un cadre méthodologique léger mais suffisant centré sur l'humain et la communication. Elles préconisent une planification adaptative, un développement évolutif, une livraison précoce et une amélioration continue, et elles encouragent des réponses flexibles au changement,.
Cette approche a été popularisée à partir de 2001 par le Manifeste pour le développement agile de logiciels. Les quatre valeurs et les douze principes adoptés dans ce manifeste sont issus d'un large éventail de méthodes dont Scrum et eXtreme Programming ,. Depuis lors, les méthodes qui s'inscrivent dans la philosophie de ce manifeste sont appelées « méthodes agiles ».
Les méthodes agiles se veulent plus pragmatiques que les méthodes traditionnelles, impliquent au maximum le demandeur (client) et permettent une grande réactivité à ses demandes. Elles reposent sur un cycle de développement itératif, incrémental et adaptatif.

### Sources

- [https://en.wikipedia.org/wiki/Agile_software_development](https://en.wikipedia.org/wiki/Agile_software_development)
- [https://fr.wikipedia.org/wiki/M%C3%A9thode_agile](https://fr.wikipedia.org/wiki/M%C3%A9thode_agile)

## Scrum (software development)

Scrum is a framework utilizing an agile mindset for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields including research, sales, marketing and advanced technologies. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team assess progress in time-boxed daily meetings of 15 minutes or less, called daily scrums.  At the end of the sprint, the team holds two further meetings: the sprint review which demonstrates the work done to stakeholders to elicit feedback, and sprint retrospective which enables the team to reflect and improve.
The software development term scrum was first used in a 1986 paper titled "The New New Product Development Game" by Hirotaka Takeuchi and Ikujiro Nonaka. The paper was published in Jan 1986 issue of Harvard Business Review. The term is borrowed from rugby, where a scrum is a formation of players. The term scrum was chosen by the paper's authors because it emphasizes teamwork.
Scrum is occasionally seen written in all-capitals, as SCRUM. While the word itself is not an acronym, its capitalized styling likely comes from an early paper by Ken Schwaber that capitalized SCRUM in its title.

### Scrum (développement)

Scrum est un framework ou cadre de développement de produits logiciels complexes. Il est défini par ses créateurs comme un « cadre de travail holistique itératif qui se concentre sur les buts communs en livrant de manière productive et créative des produits de la plus grande valeur possible ». Scrum est considéré comme un groupe de pratiques répondant pour la plupart aux préconisations du Manifeste agile.
Scrum s'appuie sur le découpage d'un projet en « boîtes de temps », nommées sprints (« pointes de vitesse »). Les sprints peuvent durer entre quelques heures et un mois (avec un sprint médian à deux semaines). Chaque sprint commence par une estimation suivie d'une planification opérationnelle. Le sprint se termine par une démonstration de ce qui a été achevé. Avant de démarrer un nouveau sprint, l'équipe réalise une rétrospective. Cette technique analyse le déroulement du sprint achevé, afin d'améliorer ses pratiques. Le flux de travail de l'équipe de développement est facilité par son auto-organisation, il n'y aura donc pas de gestionnaire de projet.
La création de frameworks de développement logiciels hybrides couplant Scrum et d'autres frameworks est commune puisque Scrum ne couvre pas le cycle de développement de produit. Par exemple, on pourra utiliser des pratiques issues de l'extreme programming, de la phase de construction structurée de la méthode RAD, ou un ensemble de pratiques de qualité du logiciel issues du vécu de l'équipe projet.
La métaphore du scrum (la « mêlée du rugby ») apparaît pour la première fois en 1986 dans une publication de Hirotaka Takeuchi et Ikujiro Nonaka, intitulée The New New Product Development Game, qui s'appliquait à l'époque au monde industriel. Ils y décrivent, sous le nom de rugby approach (« méthode du rugby »), une nouvelle méthode holistique qui augmente vitesse et flexibilité dans le développement de nouveaux produits logiciels. L'ensemble du développement est réalisé itérativement par une équipe multidisciplinaire en passant par différentes phases  et que les phases et itérations peuvent se chevaucher fortement . Ils ont comparé cette nouvelle méthode au rugby à XV, où les membres de l'équipe essayent d'aller, en bloc, jusqu'au bout, en se passant et repassant la balle.

### Sources

- [https://en.wikipedia.org/wiki/Scrum_(software_development)](https://en.wikipedia.org/wiki/Scrum_(software_development))
- [https://fr.wikipedia.org/wiki/Scrum_(d%C3%A9veloppement)](https://fr.wikipedia.org/wiki/Scrum_(d%C3%A9veloppement))

## DevOps

DevOps is a set of practices that combines software development (Dev) and IT operations (Ops). It aims to shorten the systems development life cycle and provide continuous delivery with high software quality. DevOps is complementary with Agile software development; several DevOps aspects came from the Agile methodology.
Other than it being a cross-functional combination of the terms and concepts for "development" and "operations," academics and practitioners have not developed a unique definition for the term "DevOps". The idea behind this practice is to make delivery teams responsible for the production issues and fixes, whether legacy or new. In traditional practices, delivery would only be responsible for the changes put in by them, within the warranty period.
From an academic perspective, Len Bass, Ingo Weber, and Liming Zhu—three computer science researchers from the CSIRO and the Software Engineering Institute—suggested defining DevOps as "a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality".

### Devops

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
Le devops — ou DevOps (selon la graphie habituellement utilisée en langue anglaise) — est un mouvement en ingénierie informatique et une pratique technique visant à l'unification du développement logiciel (dev) et de l'administration des infrastructures informatiques (ops), notamment l'administration système.

### Sources

- [https://en.wikipedia.org/wiki/DevOps](https://en.wikipedia.org/wiki/DevOps)
- [https://fr.wikipedia.org/wiki/Devops](https://fr.wikipedia.org/wiki/Devops)

## JSON Web Token

JSON Web Token (JWT, pronounced /dʒɒt/, same as the word "jot") is a proposed Internet standard for creating data with optional signature and/or optional encryption whose payload holds JSON that asserts some number of claims. The tokens are signed either using a private secret or a public/private key. For example, a server could generate a token that has the claim "logged in as admin" and provide that to a client. The client could then use that token to prove that it is logged in as admin. The tokens can be signed by one party's private key (usually the server's) so that party can subsequently verify the token is legitimate. If the other party, by some suitable and trustworthy means, is in possession of the corresponding public key, they too are able to verify the token's legitimacy. The tokens are designed to be compact, URL-safe, and usable especially in a web-browser single-sign-on (SSO) context. JWT claims can typically be used to pass identity of authenticated users between an identity provider and a service provider, or any other type of claims as required by business processes.
JWT relies on other JSON-based standards: JSON Web Signature and JSON Web Encryption.
HS256 indicates that this token is signed using HMAC-SHA256.

### JSON Web Token

JSON Web Token (JWT) est un standard ouvert défini dans la RFC 7519. Il permet l'échange sécurisé de jetons (tokens) entre plusieurs parties. Cette sécurité de l’échange se traduit par la vérification de l’intégrité des données à l’aide d’une signature numérique. Elle s’effectue par l'algorithme HMAC ou RSA.
Un jeton se compose de trois parties :
Il existe des outils en ligne permettant de les décrypter.

### Sources

- [https://en.wikipedia.org/wiki/JSON_Web_Token](https://en.wikipedia.org/wiki/JSON_Web_Token)
- [https://fr.wikipedia.org/wiki/JSON_Web_Token](https://fr.wikipedia.org/wiki/JSON_Web_Token)

## Software as a service

Software as a service (SaaS /sæs/) is a software licensing and delivery model in which software is licensed on a subscription basis and is centrally hosted. It is sometimes referred to as "on-demand software", and was formerly referred to as "software plus services" by Microsoft.
SaaS applications are also known as on-demand software and Web-based/Web-hosted software.
SaaS is considered to be part of cloud computing, along with infrastructure as a service (IaaS), platform as a service (PaaS), desktop as a service (DaaS), managed software as a service (MSaaS), mobile backend as a service (MBaaS), datacenter as a service (DCaaS), and information technology management as a service (ITMaaS).
SaaS apps are typically accessed by users using a thin client, e.g. via a web browser.  SaaS has become a common delivery model for many business applications, including office software, messaging software, payroll processing software, DBMS software, management software, CAD software, development software, gamification, virtualization, accounting, collaboration, customer relationship management (CRM), management information systems (MIS), enterprise resource planning (ERP), invoicing, field service management, human resource management (HRM), talent acquisition, learning management systems, content management (CM), geographic information systems (GIS), and service desk management.
SaaS has been incorporated into the strategy of nearly all leading enterprise software companies.
According to a Gartner estimate, SaaS sales in 2018 were expected to grow 23% to $72 billion.

### Software as a service

Logiciel en tant que serviceLe software as a service (SaaS) ou logiciel en tant que service, est un modèle d'exploitation commerciale des logiciels dans lequel ceux-ci sont installés sur des serveurs distants plutôt que sur la machine de l'utilisateur. Les clients ne paient pas de licence d'utilisation pour une version, mais utilisent librement le service en ligne ou, plus généralement, payent un abonnement.
Les principales applications actuelles de ce modèle sont :
Le logiciel en tant que service (SaaS) est donc la livraison conjointe de moyens, de services et de savoir-faire qui permettent aux entreprises d'externaliser intégralement un aspect de leur système d'information (messagerie, sécurité…) et de l'assimiler à un coût de fonctionnement plutôt qu'à un investissement. Le contrat de services est essentiel pour définir le niveau de qualité de service (SLA).
Le logiciel en tant que service (SaaS) peut être vu comme l'équivalent commercial de l'architecture orientée service (SOA).

### Sources

- [https://en.wikipedia.org/wiki/Software_as_a_service](https://en.wikipedia.org/wiki/Software_as_a_service)
- [https://fr.wikipedia.org/wiki/Software_as_a_service](https://fr.wikipedia.org/wiki/Software_as_a_service)

## Cryptographic hash function

A cryptographic hash function (CHF) is a mathematical algorithm that maps data of arbitrary size (often called the "message") to a bit array of a fixed size (the "hash value", "hash", or "message digest"). It is a one-way function, that is, a function which is practically infeasible to invert or reverse the computation. Ideally, the only way to find a message that produces a given hash is to attempt a brute-force search of possible inputs to see if they produce a match, or use a rainbow table of matched hashes. Cryptographic hash functions are a basic tool of modern cryptography.
The ideal cryptographic hash function has the following main properties:
Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.
Most cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value.

### Fonction de hachage cryptographique

Une fonction de hachage cryptographique est une fonction de hachage qui, à une donnée de taille arbitraire, associe une image de taille fixe, et dont une propriété essentielle est qu'elle est pratiquement impossible à inverser, c'est-à-dire que si l'image d'une donnée par la fonction se calcule très efficacement, le calcul inverse d'une donnée d'entrée ayant pour image une certaine valeur se révèle impossible sur le plan pratique. Pour cette raison, on dit d'une telle fonction qu'elle est à sens unique.
En raison de leur ubiquité, ces fonctions à sens unique ont été appelées les chevaux de trait de la cryptographie moderne. La donnée d'entrée de ces fonctions est souvent appelée message ; la valeur de sortie est souvent appelée valeur de hachage, empreinte numérique, empreinte, ou encore haché (en anglais, message digest ou digest, hash).
Une fonction de hachage cryptographique idéale possède les quatre propriétés suivantes :
Les fonctions de hachage cryptographiques sont une primitive de cryptographie symétrique, mais forment une brique de base largement utilisées dans la conception de schémas cryptographiques (aussi bien symétriques ou asymétrique), notamment dans les signatures numériques, les codes d'authentification de message et les autres formes d'authentification.

### Sources

- [https://en.wikipedia.org/wiki/Cryptographic_hash_function](https://en.wikipedia.org/wiki/Cryptographic_hash_function)
- [https://fr.wikipedia.org/wiki/Fonction_de_hachage_cryptographique](https://fr.wikipedia.org/wiki/Fonction_de_hachage_cryptographique)

## bcrypt

bcrypt is a password-hashing function designed by Niels Provos and David Mazières, based on the Blowfish cipher and presented at USENIX in 1999. Besides incorporating a salt to protect against rainbow table attacks, bcrypt is an adaptive function: over time, the iteration count can be increased to make it slower, so it remains resistant to brute-force search attacks even with increasing computation power.
The bcrypt function is the default password hash algorithm for OpenBSD and was the default for some Linux distributions such as SUSE Linux.
There are implementations of bcrypt for C, C++, C#, Elixir, Go, Java, JavaScript, Perl, PHP, Python, Ruby, and other languages.
Blowfish is notable among block ciphers for its expensive key setup phase.  It starts off with subkeys in a standard state, then uses this state to perform a block encryption using part of the key, and uses the result of that encryption (which is more accurate at hashing) to replace some of the subkeys.  Then it uses this modified state to encrypt another part of the key, and uses the result to replace more of the subkeys.  It proceeds in this fashion, using a progressively modified state to hash the key and replace bits of state, until all subkeys have been set.

### Bcrypt

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
bcrypt est une fonction de hachage créée par Niels Provos et David Mazières. Elle est basée sur l'algorithme de chiffrement Blowfish et a été présentée lors de USENIX en 1999. En plus de l'utilisation d'un sel pour se protéger des attaques par table arc-en-ciel (rainbow table), bcrypt est une fonction adaptative, c'est-à-dire que l'on peut augmenter le nombre d'itérations pour la rendre plus lente. Ainsi elle continue à être résistante aux attaques par force brute malgré l'augmentation de la puissance de calcul.
Blowfish est un algorithme de chiffrement par bloc notable pour sa phase d'établissement de clef relativement coûteuse. bcrypt utilise cette propriété et va plus loin. Provos et Mazières ont conçu un nouvel algorithme d'établissement des clefs nommé Eksblowfish (pour Expensive Key Schedule Blowfish). Dans cet algorithme, une première phase consiste à créer les sous-clefs grâce à la clef et au sel. Ensuite un certain nombre de tours de l'algorithme standard blowfish sont appliqués avec alternativement le sel et la clef. Chaque tour commence avec l'état des sous-clefs du tour précédent. Cela ne rend pas l'algorithme plus puissant que la version standard de blowfish, mais on peut choisir le nombre d'itérations ce qui le rend arbitrairement lent et contribue à dissuader les attaques par table arc-en-ciel et par force brute.

### Sources

- [https://en.wikipedia.org/wiki/Bcrypt](https://en.wikipedia.org/wiki/Bcrypt)
- [https://fr.wikipedia.org/wiki/Bcrypt](https://fr.wikipedia.org/wiki/Bcrypt)

## Cipher

In cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure. An alternative, less common term is encipherment. To encipher or encode is to convert information into cipher or code. In common parlance, "cipher" is synonymous with "code", as they are both a set of steps that encrypt a message; however, the concepts are distinct in cryptography, especially classical cryptography.
Codes generally substitute different length strings of character in the output, while ciphers generally substitute the same number of characters as are input.  There are exceptions and some cipher systems may use slightly more, or fewer, characters when output versus the number that were input.
Codes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase.  For example, "UQJHSE" could be the code for "Proceed to the following coordinates." When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.
The operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a cryptovariable). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.

### Chiffre (cryptologie)

Vous pouvez améliorer la vérifiabilité en associant ces informations à des références à l'aide d'appels de notes.
En cryptologie, un chiffre est une manière secrète d'écrire un message à transmettre, au moyen de caractères et de signes disposés selon une convention convenue au préalable. Plus précisément, le chiffre est l’ensemble des conventions et des symboles (lettres, nombres, signes, etc.) employés pour remplacer chaque lettre du message à rendre secret. Avec un chiffre, on transforme un message en clair en message en chiffres, ou message chiffré, ou encore cryptogramme.
Dans l'armée ou la diplomatie, le service du chiffre est l'unité chargée de transmettre et de recevoir la correspondance secrète.
Comme la nécessité d'échanger des messages confidentiels est aussi vieille que l'écriture, des chiffres ont existé depuis l'Antiquité. Exemples : le chiffre de César, le chiffre de Trithémius, le chiffre de Vigenère, le chiffre de Vernam, le chiffre des templiers.

### Sources

- [https://en.wikipedia.org/wiki/Cipher](https://en.wikipedia.org/wiki/Cipher)
- [https://fr.wikipedia.org/wiki/Chiffre_(cryptologie)](https://fr.wikipedia.org/wiki/Chiffre_(cryptologie))

## Algorithm

In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ (listen)) is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of specific problems or to perform a computation. Algorithms are always unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. In contrast, a heuristic is a  technique used in problem solving that uses practical methods and/or various estimates in order to produce solutions that may not be optimal but are sufficient given the circumstances.  
As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.
The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in 240 BC in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.

### Algorithme

Un algorithme est une suite finie et non ambiguë d’opérations ou d'instructions permettant de résoudre une classe de problèmes.
Le mot algorithme vient du nom d'un mathématicien perse du IXe siècle, Al-Khwârizmî (en arabe : الخوارزمي).
Le domaine qui étudie les algorithmes est appelé l'algorithmique. On retrouve aujourd'hui des algorithmes dans de nombreuses applications telles que le fonctionnement des ordinateurs, la cryptographie, le routage d'informations, la planification et l'utilisation optimale des ressources, le traitement d'images, le traitement de textes, la bio-informatique, etc.
Un algorithme est une méthode générale pour résoudre un type de problèmes. Il est dit correct lorsque, pour chaque instance du problème, il se termine en produisant la bonne sortie, c'est-à-dire qu'il résout le problème posé.

### Sources

- [https://en.wikipedia.org/wiki/Algorithm](https://en.wikipedia.org/wiki/Algorithm)
- [https://fr.wikipedia.org/wiki/Algorithme](https://fr.wikipedia.org/wiki/Algorithme)

## Parallel computing

Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.
Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.
Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.

### Parallélisme (informatique)

En informatique, le parallélisme consiste à mettre en œuvre des architectures d'électronique numérique permettant de traiter des informations de manière simultanée, ainsi que les algorithmes spécialisés pour celles-ci. Ces techniques ont pour but de réaliser le plus grand nombre d'opérations en un temps le plus petit possible.
Les architectures parallèles sont devenues le paradigme dominant pour tous les ordinateurs depuis les années 2000. En effet, la vitesse de traitement qui est liée à l'augmentation de la fréquence des processeurs connait des limites. La création de processeurs multi-cœurs, traitant plusieurs instructions en même temps au sein du même composant, résout ce dilemme pour les machines de bureau depuis le milieu des années 2000.
Pour être efficaces, les méthodes utilisées pour la programmation des différentes tâches qui constituent un programme sont spécifiques à ce mode de calcul, c'est-à-dire que les programmes doivent être réalisés avec cette optique. Ces méthodes ont initialement été développées de manière théorique et sur des superordinateurs, qui étaient à une période les seuls à compter de nombreux processeurs, mais sont de plus en plus volontiers utilisées par les développeurs de logiciel du fait de l'omniprésence de telles architectures.
Certains types de calculs se prêtent particulièrement bien à la parallélisation : la dynamique des fluides, les prédictions météorologiques, la modélisation et simulation de problèmes de dimensions plus grandes, le traitement de l'information et l'exploration de données, le décryptage de messages, la recherche de mots de passe, le traitement d'images ou la fabrication d'images de synthèse, tels que le lancer de rayon, l'intelligence artificielle et la fabrication automatisée. Initialement, c'est dans le domaine des supercalculateurs que le parallélisme a été utilisé, à des fins scientifiques.

### Sources

- [https://en.wikipedia.org/wiki/Parallel_computing](https://en.wikipedia.org/wiki/Parallel_computing)
- [https://fr.wikipedia.org/wiki/Parall%C3%A9lisme_(informatique)](https://fr.wikipedia.org/wiki/Parall%C3%A9lisme_(informatique))

## Concurrency (computer science)

In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or at the same time simultaneously partial order, without affecting the final outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability of a program, algorithm, or problem into order-independent or partially-ordered components or units of computation.
According to Rob Pike, concurrency is the composition of independently executing computations, and concurrency is not parallelism: concurrency is about dealing with lots of things at once but parallelism is about doing lots of things at once. Concurrency is about structure, parallelism is about execution, concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.
A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi, the parallel random-access machine model, the actor model and the Reo Coordination Language.
As Leslie Lamport (2015) notes, "While concurrent program execution had been considered for years, the computer science of concurrency began with Edsger Dijkstra's seminal 1965 paper that introduced the mutual exclusion problem. ... The ensuing decades have seen a huge growth of interest in concurrency—particularly in distributed systems. Looking back at the origins of the field, what stands out is the fundamental role played by Edsger Dijkstra".

### Sources

- [https://en.wikipedia.org/wiki/Concurrency_(computer_science)](https://en.wikipedia.org/wiki/Concurrency_(computer_science))

## Programming language

A programming language is a formal language comprising a set of strings that produce various kinds of machine code output. Programming languages are one kind of computer language, and are used in computer programming to implement algorithms.
Most programming languages consist of instructions for computers. There are programmable machines that use a set of specific instructions, rather than general programming languages. Since the early 1800s, programs have been used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos. The programs for these machines (such as a player piano's scrolls) did not produce different behavior in response to different inputs or conditions.
Thousands of different programming languages have been created, and more are being created every year. Many programming languages are written in an imperative form (i.e., as a sequence of operations to perform) while other languages use the declarative form (i.e. the desired result is specified, not how to achieve it).

### Langage de programmation

Un langage de programmation est une notation conventionnelle destinée à formuler des algorithmes et produire des programmes informatiques qui les appliquent. D'une manière similaire à une langue naturelle, un langage de programmation est composé d'un alphabet, d'un vocabulaire, de règles de grammaire, de significations, mais aussi d'un environnement de traduction censé rendre sa syntaxe compréhensible par la machine,.
Les langages de programmation permettent de décrire d'une part les structures des données qui seront manipulées par l'appareil informatique, et d'autre part d'indiquer comment sont effectuées les manipulations, selon quels algorithmes. Ils servent de moyens de communication par lesquels le programmeur communique avec l'ordinateur, mais aussi avec d'autres programmeurs ; les programmes étant d'ordinaire écrits, lus, compris et modifiés par une équipe de programmeurs.
Un langage de programmation est mis en œuvre par un traducteur automatique : compilateur ou interprète. Un compilateur est un programme informatique qui transforme dans un premier temps un code source écrit dans un langage de programmation donné en un code cible qui pourra être directement exécuté par un ordinateur, à savoir un programme en langage machine ou en code intermédiaire, tandis que l’interprète réalise cette traduction « à la volée ».
Les langages de programmation offrent différentes possibilités d'abstraction et une notation proche de l'algèbre, permettant de décrire de manière concise et facile à saisir les opérations de manipulation de données et l'évolution du déroulement du programme en fonction des situations. La possibilité d'écriture abstraite libère l'esprit du programmeur d'un travail superflu, notamment de prise en compte des spécificités du matériel informatique, et lui permet ainsi de se concentrer sur des problèmes plus avancés.

### Sources

- [https://en.wikipedia.org/wiki/Programming_language](https://en.wikipedia.org/wiki/Programming_language)
- [https://fr.wikipedia.org/wiki/Langage_de_programmation](https://fr.wikipedia.org/wiki/Langage_de_programmation)

## Garbage collection (computer science)

In computer science, garbage collection (GC) is a form of automatic memory management. The garbage collector attempts to reclaim memory which was allocated by the program, but is no longer referenced—also called garbage. Garbage collection was invented by American computer scientist John McCarthy around 1959 to simplify manual memory management in Lisp.
Garbage collection relieves the programmer from performing  manual memory management where the programmer specifies what objects to deallocate and return to the memory system and when to do so. Other similar techniques include stack allocation, region inference, memory ownership, and combinations of multiple techniques. Garbage collection may take a significant proportion of total processing time in a program and, as a result, can have significant influence on performance.
Resources other than memory, such as network sockets, database handles, user interaction windows, file and device descriptors, are not typically handled by garbage collection. Methods for managing such resources, particularly destructors, may suffice to manage memory as well, leaving no need for GC. Some GC systems allow such other resources to be associated with a region of memory that, when collected, causes the work of reclaiming these resources.

### Ramasse-miettes (informatique)

Un ramasse-miettes, ou récupérateur de mémoire, ou glaneur de cellules (en anglais garbage collector, abrégé en GC),  est un sous-système informatique de gestion automatique de la mémoire. Il est responsable du recyclage de la mémoire préalablement allouée puis inutilisée.
Lorsqu'un système dispose d'un ramasse-miettes, ce dernier fait généralement partie de l'environnement d'exécution associé à un langage de programmation particulier. Le ramassage des miettes a été inventé par John McCarthy comme faisant partie du premier système Lisp et le premier programme de ramasse-miettes a été écrit par son étudiant Daniel J. Edwards.
Le terme français de ramasse-miettes apparaît vers 1970 dans les communications de Claude Pair aux écoles d'été de l'AFCET ainsi qu'à la même époque dans ses cours d'informatique à l'université de Nancy. Il est rapidement adopté les années suivantes par Jean-Claude Derniame (Nancy), Louis Bazerque (Toulouse), Jean Cea (Nice), Jean-Pierre Verjus (Rennes), Claude Hans (Grenoble), Olivier Lecarme (Montréal) et Jacques Arsac (Paris).
Le principe de base de la récupération automatique de la mémoire est assez simple ; il consiste

### Sources

- [https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science))
- [https://fr.wikipedia.org/wiki/Ramasse-miettes_(informatique)](https://fr.wikipedia.org/wiki/Ramasse-miettes_(informatique))

## Structural type system

A structural type system (or property-based type system) is a major class of type system in which type compatibility and equivalence are determined by the type's actual structure or definition and not by other characteristics such as its name or place of declaration. Structural systems are used to determine if types are equivalent and whether a type is a subtype of another. It contrasts with nominative systems, where comparisons are based on the names of the types or explicit declarations, and duck typing, in which only the part of the structure accessed at runtime is checked for compatibility.
In structural typing, an element is considered to be compatible with another if, for each feature within the second element's type, a corresponding and identical feature exists in the first element's type.  Some languages may differ on the details, such as whether the features must match in name. This definition is not symmetric, and includes subtype compatibility. Two types are considered to be identical if each is compatible with the other.
For example, OCaml uses structural typing on methods for compatibility of object types. Go uses structural typing on methods to determine compatibility of a type with an interface. C++ template functions exhibit structural typing on type arguments. Haxe uses structural typing, but classes are not structurally subtyped.
In languages which support subtype polymorphism, a similar dichotomy can be formed based on how the subtype relationship is defined.  One type is a subtype of another if and only if it contains all the features of the base type, or subtypes thereof. The subtype may contain added features, such as members not present in the base type, or stronger invariants.

### Système structurel de types

En programmation informatique, un  système structurel de types est une classe majeure de système de types, dans laquelle la compatibilité et l'équivalence de type est déterminée par la structure du type et non par des déclarations explicites. On utilise les systèmes structuraux pour déterminer si les types sont équivalents, ou si un type est un sous-type d'un autre. Ce système est en opposition avec les systèmes nominatifs où les comparaisons sont basées sur des déclarations explicites ou sur le nom des types.
Avec le typage structurel deux objets ou termes sont considérés comme ayant des types compatibles si leurs types ont une structure identique.
Selon la sémantique du langage, cela signifie généralement que pour chaque fonctionnalité dans un type il doit y voir une fonctionnalité correspondante dans
l'autre type. Les détails changent selon les langages. Ainsi sur le fait que deux fonctionnalités doivent avoir le même nom pour être considérées identiques.
Haskell et ML sont deux langages au typage structurel. Haxe est un langage au typage structurel bien que ses classes ne soient pas structurellement sous-typées.
Dans les langages qui supportent le sous-typage polymorphique, on peut former une dichotomie similaire pour définir une relation de sous-type.
Un type est un sous-type d'un autre si et seulement il contient toutes les fonctionnalités du type de base ou de l'un de ses sous-types.
Le sous-type peut contenir des fonctionnalités additionnelles telles que des membres non présents dans le type de base ou des invariants plus forts.

### Sources

- [https://en.wikipedia.org/wiki/Structural_type_system](https://en.wikipedia.org/wiki/Structural_type_system)
- [https://fr.wikipedia.org/wiki/Syst%C3%A8me_structurel_de_types](https://fr.wikipedia.org/wiki/Syst%C3%A8me_structurel_de_types)

## Argon2

Argon2 is a key derivation function that was selected as the winner of the Password Hashing Competition in July 2015. It was designed by Alex Biryukov, Daniel Dinu, and Dmitry Khovratovich from the University of Luxembourg. The reference implementation of Argon2 is released under a Creative Commons CC0 license (i.e. public domain) or the Apache License 2.0, and provides three related versions:
All three modes allow specification by three parameters that control:
While there is no public cryptanalysis applicable to Argon2d, there are two published attacks on the Argon2i function. The first attack is applicable only to the old version of Argon2i, while the second has been extended to the latest version (1.3)
The first attack shows that it is possible to compute a single-pass Argon2i function using between a quarter and a fifth of the desired space with no time penalty, and compute a multiple-pass Argon2i using only N/e < N/2.71 space with no time penalty. According to the Argon2 authors, this attack vector was fixed in version 1.3.

### Argon2

Vous pouvez partager vos connaissances en l’améliorant (comment ?) selon les recommandations des projets correspondants.
Argon2 est une fonction de dérivation de clé, gagnante de la Password Hashing Competition en juillet 2015,. Elle a été conçue par Alex Biryukov, Daniel Dinu et Dmitry Khovratovich de l'Université de Luxembourg et publié sous licence Creative Commons CC0. Cette fonction se décline en deux versions : Argon2d, conçue pour résister aux attaques par GPU devenu de plus en plus puissant en calcul, et Argon2i optimisée contre les attaques par canal auxiliaire.
Cette fonction prend trois paramètres permettant de contrôler son temps d’exécution, la mémoire requise et le degré de parallélisme des calculs.

En 2017, pour la version 7.2 de PHP, Argon2 est maintenant intégré dans le cœur de PHP sur la fonction password_hash() avec la constante PASSWORD_ARGON2I, mais cette fonction de hash n'est pas définie comme celle par défaut, qui reste toujours Bcrypt.

### Sources

- [https://en.wikipedia.org/wiki/Argon2](https://en.wikipedia.org/wiki/Argon2)
- [https://fr.wikipedia.org/wiki/Argon2](https://fr.wikipedia.org/wiki/Argon2)

## Variable (computer science)

In computer programming, a variable or scalar is a storage location (identified by a memory address) paired with an associated symbolic name, which contains some known or unknown quantity of information referred to as a value or in easy terms, a variable is a container for different types of data (like integer, float, String and etc...). The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution.

Variables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation.
A variable's storage location may be referenced by several different identifiers, a situation known as aliasing. Assigning a value to the variable using one of the identifiers will change the value that can be accessed through the other identifiers.
Compilers have to replace variables' symbolic names with the actual locations of the data. While a variable's name, type, and location often remain fixed, the data stored in the location may be changed during program execution.

### Variable (informatique)

En informatique, les variables sont des symboles qui associent un nom (l'identifiant) à une valeur. Dans la plupart des langages et notamment les plus courants,  les variables peuvent changer de valeur au cours du temps (dynamique). Dans les langages de certains paradigmes, notamment la programmation fonctionnelle, leur valeur est au contraire figée dans le temps (statique).
Contrairement à une variable, une constante est un identificateur associé à une valeur fixe. Syntaxiquement, cet identificateur a tous les aspects d'une variable. Cependant, il lui est affecté une valeur définitive,  c'est-à-dire constante, comme la taille d'un plateau d'échecs (8x8). Une constante contient une valeur qui peut avoir des valeurs différentes suivant les exécutions, à la manière du jeu  démineur dont le joueur peut choisir la taille du plateau.
Dans un langage de programmation, une variable est un espace de stockage[Information douteuse] pour un résultat. Cependant les possibilités[Quoi ?] d'une variable sont intimement liées au langage de programmation auquel on fait référence. Par exemple une variable en C++ aura six caractéristiques :
Toutefois on peut trouver des langages qui restreignent ces caractéristiques :

### Sources

- [https://en.wikipedia.org/wiki/Variable_(computer_science)](https://en.wikipedia.org/wiki/Variable_(computer_science))
- [https://fr.wikipedia.org/wiki/Variable_(informatique)](https://fr.wikipedia.org/wiki/Variable_(informatique))

## Memory address

In computing, a memory address is a reference to a specific memory location used at various levels by software and hardware. Memory addresses are fixed-length sequences of digits conventionally displayed and manipulated as unsigned integers. Such numerical semantic bases itself upon features of CPU (such as the instruction pointer and incremental address registers), as well upon use of the memory like an array endorsed by various programming languages.
A digital computer's main memory consists of many memory locations. Each memory location has a physical address which is a code. The CPU (or other device) can use the code to access the corresponding memory location. Generally only system software, i.e. the BIOS, operating systems, and some specialized utility programs (e.g., memory testers), address physical memory using machine code operands or processor registers, instructing the CPU to direct a hardware device, called the memory controller, to use the memory bus or system bus, or separate control, address and data busses, to execute the program's commands.   The memory controllers' bus consists of a number of parallel lines, each represented by a binary digit (bit).  The width of the bus, and thus the number of addressable storage units, and the number of bits in each unit, varies among computers.
A computer program uses memory addresses to execute machine code, and to store and retrieve data.  In early computers logical and physical addresses corresponded, but since the introduction of virtual memory most application programs do not have a knowledge of physical addresses.  Rather, they address logical addresses, or virtual addresses, using the computer's memory management unit and operating system memory mapping; see below.
Most modern computers are byte-addressable. Each address identifies a single byte (eight bits) of storage. Data larger than a single byte may be stored in a sequence of consecutive addresses.  There exist word-addressable computers, where the minimal addressable storage unit is exactly the processor's word. For example, the Data General Nova minicomputer, and the Texas Instruments TMS9900 and National Semiconductor IMP-16 microcomputers used 16 bit words, and there were many 36-bit mainframe computers (e.g., PDP-10) which used 18-bit word addressing, not byte addressing, giving an address space of 218 36-bit words, approximately 1 megabyte of storage.  The efficiency of addressing of memory depends on the bit size of the bus used for addresses – the more bits used, the more addresses are available to the computer.  For example, an 8-bit-byte-addressable machine with a 20-bit address bus (e.g. Intel 8086) can address 220 (1,048,576) memory locations, or one MiB of memory, while a 32-bit bus (e.g. Intel 80386) addresses 232 (4,294,967,296) locations, or a 4 GiB address space.  In contrast, a 36-bit word-addressable machine with an 18-bit address bus addresses only 218 (262,144) 36-bit locations (9,437,184 bits), equivalent to 1,179,648 8-bit bytes, or 1152 KB, or 1.125 MiB—slightly more than the 8086.

### Adressage mémoire

L’adressage mémoire est, en électronique et en informatique, la façon dont se fait l'accès aux données conservées en mémoire. Une adresse mémoire est un nombre entier naturel (rarement une autre sorte d'identifiant) qui désigne une zone particulière de la mémoire, ou juste le début d'une zone. Le plus souvent, une donnée peut être lue ou écrite. La mémoire peut être temporaire (mémoire vive) pour le travail ou au contraire durable (mémoire non volatile) pour le stockage.
L'immense majorité des programmes informatiques fait usage de la mémoire lors de son exécution. Le stockage temporaire d'une donnée (une variable) implique l'accès à la mémoire vive : dans ce cas, c'est le processeur qui écrit sur le bus. Lors d'un accès à un composant DMA (par exemple un disque dur), c'est le contrôleur DMA qui, cette fois, agit directement sur le bus système.
Les ordinateurs assez puissants pour faire fonctionner de nombreux programmes simultanément sont généralement équipés d'une unité de gestion de mémoire (MMU) capable de traduire à la volée les adresses mémoire envoyées par le processeur à la mémoire. C'est notamment le cas des ordinateurs personnels depuis les années 1990. Lorsqu'une telle unité est en fonction, les adresses émises par le processeur sont alors des adresses virtuelles, et les adresses traduites qui arrivent à la mémoire sont des adresses physiques.
Le système d'exploitation peut notamment programmer l'unité de gestion de mémoire pour :

### Sources

- [https://en.wikipedia.org/wiki/Memory_address](https://en.wikipedia.org/wiki/Memory_address)
- [https://fr.wikipedia.org/wiki/Adressage_m%C3%A9moire](https://fr.wikipedia.org/wiki/Adressage_m%C3%A9moire)

## Integer (computer science)

In computer science, an integer is a datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware nearly always provide a way to represent a processor register or memory address as an integer.
The value of an item with an integral type is the mathematical integer that it corresponds to. Integral types may be unsigned (capable of representing only non-negative integers) or signed (capable of representing negative integers as well).
An integer value is typically specified in the source code of a program as a sequence of digits optionally prefixed with + or −. Some programming languages allow other notations, such as hexadecimal (base 16) or octal (base 8). Some programming languages also permit digit group separators.
The internal representation of this datum is the way the value is stored in the computer's memory. Unlike mathematical integers, a typical datum in a computer has some minimal and maximum possible value.

### Entier (informatique)

En informatique, un entier est un type de donnée qui représente un sous-ensemble fini de nombres entiers relatifs. On utilise aussi le terme type de données entières (integral type data).
Certains traitements comme le recensement des États-Unis ont d'abord été effectués en utilisant une représentation décimale à l'aide de cartes perforées. Le système décimal utilise dix chiffres (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) et où leur position correspond à une puissance de 10 (1, 10, 100, 1000, etc.).
Le développement de l'informatique a ensuite conduit à l'utilisation de la représentation binaire des entiers, mais aussi de la représentation décimale, où chaque chiffre de 0 à 9 est codé en binaire (voir Binary coded decimal, IBM 1620). Autrement dit, on représente chacun des dix chiffres en binaire (sur 4 bits) pour implémenter l'arithmétique décimale.
Un ordinateur moderne utilise des tensions électriques pour gérer ses données. En théorie, plusieurs tensions pourraient être utilisées pour représenter une information[réf. nécessaire], mais en pratique on en n’utilise que deux. En effet, des problèmes de distorsion apparaitraient et empêcheraient de conserver l'information. De plus, les avancées technologiques tendent à réduire la tension afin d'éviter de chauffer les matériaux[réf. nécessaire]. On utilise donc deux niveaux de tension, un bit prend alors 2 valeurs qui correspondent à 0 et 1.

### Sources

- [https://en.wikipedia.org/wiki/Integer_(computer_science)](https://en.wikipedia.org/wiki/Integer_(computer_science))
- [https://fr.wikipedia.org/wiki/Entier_(informatique)](https://fr.wikipedia.org/wiki/Entier_(informatique))

## Data type

In computer science and computer programming, a data type or simply type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans. A data type constrains the values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored. A data type provides a set of values from which an expression (i.e. variable, function, etc.) may take its values.
Data types are used within type systems, which offer various ways of defining, implementing, and using them. Different type systems ensure varying degrees of type safety.
Almost all programming languages explicitly include the notion of data type, though different languages may use different terminology.
Common data types include:

### Type (informatique)

En programmation informatique, un type de donnée, ou simplement un type, définit la nature des valeurs que peut prendre une donnée, ainsi que les opérateurs qui peuvent lui être appliqués.
La plupart des langages de programmation de haut niveau offrent des types de base correspondant aux données qui peuvent être traitées directement — à savoir : sans conversion ou formatage préalable — par le processeur. Ces types de base sont souvent :
Les langages permettant un accès direct à la mémoire du système offrent par ailleurs le type pointeur, et un type octet.
Beaucoup proposent également un type prédéfini, string, pour les chaînes de caractères.
Les langages de haut niveau peuvent également supporter nativement des types correspondant à d'autres structures de données.

### Sources

- [https://en.wikipedia.org/wiki/Data_type](https://en.wikipedia.org/wiki/Data_type)
- [https://fr.wikipedia.org/wiki/Type_(informatique)](https://fr.wikipedia.org/wiki/Type_(informatique))

## Compiler

In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name "compiler" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.:p1
There are many different types of compilers which produce output in different useful forms. A compiler that can run on a computer whose CPU or operating system is different from the one on which the code it produces will run is called a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transcompiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.
A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.

### Compilateur

En informatique, un compilateur  est un programme qui transforme un code source en un code objet. Généralement, le code source est écrit dans un langage de programmation (le langage source), il est de haut niveau d'abstraction, et facilement compréhensible par l'humain. Le code objet est généralement écrit en langage de plus bas niveau (appelé langage cible), par exemple un langage d'assemblage ou langage machine, afin de créer un programme exécutable par une machine.
Un compilateur effectue les opérations suivantes : analyse lexicale, pré-traitement (préprocesseur), analyse syntaxique (parsing), analyse sémantique, et génération de code optimisé. La compilation est souvent suivie d'une étape d’édition des liens, pour générer un fichier exécutable. Quand le programme compilé (code objet) est exécuté sur un ordinateur dont le processeur ou le système d'exploitation est différent de celui du compilateur, on parle de compilation croisée.
On distingue deux options de compilation :
Un programme source en langage C

### Sources

- [https://en.wikipedia.org/wiki/Compiler](https://en.wikipedia.org/wiki/Compiler)
- [https://fr.wikipedia.org/wiki/Compilateur](https://fr.wikipedia.org/wiki/Compilateur)

## Software deployment

Software deployment is all of the activities that make a software system available for use.
The general deployment process consists of several interrelated activities with possible transitions between them. These activities can occur at the producer side or at the consumer side or both. Because every software system is unique, the precise processes or procedures within each activity can hardly be defined. Therefore, "deployment" should be interpreted as a general process that has to be customized according to specific requirements or characteristics.
When computers were extremely large, expensive, and bulky (mainframes and minicomputers), the software was often bundled together with the hardware by manufacturers. If business software needed to be installed on an existing computer, this might require an expensive, time-consuming visit by a systems architect or a consultant. For complex, on-premises installation of enterprise software today, this can still sometimes be the case.

### Sources

- [https://en.wikipedia.org/wiki/Software_deployment](https://en.wikipedia.org/wiki/Software_deployment)

## Software engineering

Software engineering is the systematic application of engineering approaches to the development of software.
When the first digital computers appeared in the early 1940s, the instructions to make them operate were wired into the machine. Practitioners quickly realized that this design was not flexible and came up with the "stored program architecture" or von Neumann architecture. Thus the division between "hardware" and "software" began with abstraction being used to deal with the complexity of computing.
Programming languages started to appear in the early 1950s and this was also another major step in abstraction. Major languages such as Fortran, ALGOL, PL/I, and COBOL were released in the late 1950s and 1960s to deal with scientific, algorithmic, and business problems respectively. David Parnas introduced the key concept of modularity and information hiding in 1972 to help programmers deal with the ever-increasing complexity of software systems.
In the 1960s, software engineering was seen as its own type of engineering. Also during that time, the development of software engineering was a struggle. It was hard to keep up with the hardware. This caused many problems for software engineers. Problems included software that over budget and past deadlines, extensive de-bugging, required lots of maintenance, unsuccessfully met the needs of consumers, or never been completed. 1968 was the year when NATO held the first Software Engineering conference where issues of software were addressed, guidelines were established, and the best practices for the development of software.

### Génie logiciel

modifier - modifier le code - modifier WikidataLe génie logiciel, l'ingénierie logicielle ou l'ingénierie du logiciel (en anglais : software engineering) est une science de génie industriel qui étudie les méthodes de travail et les bonnes pratiques des ingénieurs qui développent des logiciels. Le génie logiciel s'intéresse en particulier aux procédures systématiques qui permettent d'arriver à ce que des logiciels de grande taille correspondent aux attentes du client, soient fiables, aient un coût d'entretien réduit et de bonnes performances tout en respectant les délais et les coûts de construction.
Selon l'arrêté ministériel du 30 décembre 1983 relatif à l'enrichissement du vocabulaire de l'informatique [Journal officiel du 19 février 1984], le génie logiciel est « l'ensemble des activités de conception et de mise en œuvre des produits et des procédures tendant à rationaliser la production du logiciel et son suivi ».
Est aussi appelée génie logiciel l'ingénierie appliquée au logiciel informatique, c'est-à-dire l'activité par laquelle le code source d'un logiciel est spécifié puis produit et déployé. Le génie logiciel touche au cycle de vie des logiciels. Toutes les phases de la création d'un logiciel informatique y sont enseignées : l'analyse du besoin, l'élaboration des spécifications, la conceptualisation du mécanisme interne au logiciel ainsi que les techniques de programmation, le développement, la phase de test et finalement la maintenance.

### Sources

- [https://en.wikipedia.org/wiki/Software_engineering](https://en.wikipedia.org/wiki/Software_engineering)
- [https://fr.wikipedia.org/wiki/G%C3%A9nie_logiciel](https://fr.wikipedia.org/wiki/G%C3%A9nie_logiciel)

## von Neumann architecture

The von Neumann architecture—also known as the von Neumann model or Princeton architecture—is a computer architecture based on a 1945 description by John von Neumann and others in the First Draft of a Report on the EDVAC. That document describes a design architecture for an electronic digital computer with these components:
The term "von Neumann architecture" has evolved to mean any stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus. This is referred to as the von Neumann bottleneck and often limits the performance of the system.

### Architecture de von Neumann

L’architecture dite architecture de von Neumann est un modèle pour un ordinateur qui utilise une structure de stockage unique pour conserver à la fois les instructions et les données demandées ou produites par le calcul. De telles machines sont aussi connues sous le nom d’ordinateur à programme enregistré. La séparation entre le stockage et le processeur est implicite dans ce modèle.
Cette architecture est appelée ainsi en référence au mathématicien John von Neumann qui a élaboré en juin 1945 dans le cadre du projet EDVAC la première description d’un ordinateur dont le programme est stocké dans sa mémoire.
Alan Turing, John von Neumann ainsi que John William Mauchly et John Eckert (pendant leurs travaux sur l’ENIAC), ont utilisé ce concept indépendamment.
Herman Goldstine (un collègue de John von Neumann) fit circuler une description inachevée, intitulée «Première ébauche d'un rapport sur EDVAC» basée sur les travaux d'Eckert et Mauchly sous le seul nom de von Neumann.

### Sources

- [https://en.wikipedia.org/wiki/Von_Neumann_architecture](https://en.wikipedia.org/wiki/Von_Neumann_architecture)
- [https://fr.wikipedia.org/wiki/Architecture_de_von_Neumann](https://fr.wikipedia.org/wiki/Architecture_de_von_Neumann)

## Processor register

A processor register is a quickly accessible location available to a computer's processor. Registers usually consist of a small amount of fast storage, although some registers have specific hardware functions, and may be read-only or write-only. In computer architecture, registers are typically addressed by mechanisms other than main memory, but may in some cases be assigned a memory address e.g. DEC PDP-10, ICT 1900.
Almost all computers, whether load/store architecture or not, load data from a larger memory into registers where it is used for arithmetic operations and is manipulated or tested by machine instructions. Manipulated data is then often stored back to main memory, either by the same instruction or by a subsequent one. Modern processors use either static or dynamic RAM as main memory, with the latter usually accessed via one or more cache levels.
Processor registers are normally at the top of the memory hierarchy, and provide the fastest way to access data. The term normally refers only to the group of registers that are directly encoded as part of an instruction, as defined by the instruction set. However, modern high-performance CPUs often have duplicates of these "architectural registers" in order to improve performance via register renaming, allowing parallel and speculative execution. Modern x86 design acquired these techniques around 1995 with the releases of Pentium Pro, Cyrix 6x86, Nx586, and AMD K5.
When a computer program accesses the same data repeatedly, this is called locality of reference.  Holding frequently used values in registers can be critical to a program's performance. Register allocation is performed either by a compiler in the code generation phase, or manually by an assembly language programmer.

### Registre de processeur

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
Un registre est un emplacement de mémoire interne à un processeur. Les registres se situent au sommet de la hiérarchie mémoire : il s'agit de la mémoire la plus rapide d'un ordinateur, mais dont le coût de fabrication est le plus élevé, car la place dans un microprocesseur est limitée.
Une architecture externe de processeur définit un ensemble de registres, dits architecturaux, qui sont accessibles par son jeu d'instructions. Ils constituent l'état externe (architectural) du processeur. Cependant, une réalisation donnée d'une architecture interne (microarchitecture) peut contenir un ensemble différent de registres, qui sont en général plus nombreux que les registres architecturaux. Ils stockent non seulement l'état externe du processeur, mais aussi celui de sa microarchitecture : valeurs opérandes, indicateurs, etc. Ce dernier état est utilisé exclusivement par la microarchitecture, et n'est pas visible par le jeu d'instructions (architecture). Un processeur peut contenir plusieurs centaines de registres, mais, à titre d’exemple, un processeur Intel 32 bits en contient seize. Chaque registre a une capacité de 8, 16, 32 ou 64 bits (couramment la taille d’un bus).

### Sources

- [https://en.wikipedia.org/wiki/Processor_register](https://en.wikipedia.org/wiki/Processor_register)
- [https://fr.wikipedia.org/wiki/Registre_de_processeur](https://fr.wikipedia.org/wiki/Registre_de_processeur)

## SIMD

Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. Such machines exploit data level parallelism, but not concurrency: there are simultaneous (parallel) computations, but only a single process (instruction) at a given moment. SIMD is particularly applicable to common tasks such as adjusting the contrast in a digital image or adjusting the volume of digital audio. Most modern CPU designs include SIMD instructions to improve the performance of multimedia use. SIMD is not to be confused with SIMT, which utilizes threads.
The first use of SIMD instructions was in the ILLIAC IV, which was completed in 1966.
SIMD was the basis for vector supercomputers of the early 1970s such as the CDC Star-100 and the Texas Instruments ASC, which could operate on a "vector" of data with a single instruction. Vector processing was especially popularized by Cray in the 1970s and 1980s. Vector-processing architectures are now considered separate from SIMD computers, based on the fact that vector computers processed the vectors one word at a time through pipelined processors (though still based on a single instruction), whereas modern SIMD computers process all elements of the vector simultaneously.
The first era of modern SIMD computers was characterized by massively parallel processing-style supercomputers such as the Thinking Machines CM-1 and CM-2. These computers had many limited-functionality processors that would work in parallel. For example, each of 65,536 single-bit processors in a Thinking Machines CM-2 would execute the same instruction at the same time, allowing, for instance, to logically combine 65,536 pairs of bits at a time, using a hypercube-connected network or processor-dedicated RAM to find its operands. Supercomputing moved away from the SIMD approach when inexpensive scalar MIMD approaches based on commodity processors such as the Intel i860 XP became more powerful, and interest in SIMD waned.

### Single instruction multiple data

Vous pouvez partager vos connaissances en l’améliorant (comment ?) selon les recommandations des projets correspondants.
Single Instruction on Multiple Data (signifiant en anglais : « instruction unique, données multiples »), ou SIMD, est une des quatre catégories d'architecture définies par la taxonomie de Flynn en 1966 et désigne un mode de fonctionnement des ordinateurs dotés de capacités de parallélisme. Dans ce mode, la même instruction est appliquée simultanément à plusieurs données pour produire plusieurs résultats.
On utilise cette abréviation par opposition à SISD (Single Instruction on Single Data), le fonctionnement traditionnel, et MIMD (Multiple Instructions on Multiple Data), le fonctionnement avec plusieurs processeurs aux mémoires indépendantes.

### Sources

- [https://en.wikipedia.org/wiki/SIMD](https://en.wikipedia.org/wiki/SIMD)
- [https://fr.wikipedia.org/wiki/Single_instruction_multiple_data](https://fr.wikipedia.org/wiki/Single_instruction_multiple_data)

## SWAR

SIMD within a register (SWAR) is a technique for performing parallel operations on data contained in a processor register. SIMD stands for single instruction, multiple data.
Many modern general-purpose computer processors have some provisions for SIMD, in the form of a group of registers and instructions to make use of them. SWAR refers to the use of those registers and instructions, as opposed to using specialized processing engines designed to be better at SIMD operations. It also refers to the use of SIMD with general-purpose registers and instructions that were not meant to do it at the time, by way of various novel software tricks.
A SWAR architecture is one that includes instructions explicitly intended to perform parallel operations across data that is stored in the independent subwords or fields of a register.  A SWAR-capable architecture is one that includes a set of instructions that is sufficient to allow data stored in these fields to be treated independently even though the architecture does not include instructions that are explicitly intended for that purpose.
An early example of a SWAR architecture was the Intel Pentium with MMX, which implemented the MMX extension set.  The Intel Pentium, by contrast, did not include such instructions, but could still act as a SWAR architecture through careful hand-coding or compiler techniques.

### SWAR

Vous pouvez partager vos connaissances en l’améliorant (comment ?) selon les recommandations des projets correspondants.
En informatique, SWAR est un acronyme signifiant « SIMD Within A Register ». Ce terme a été inventé en 1996 pour faire la différence entre le parallélisme interne à un processeur (comme AltiVec, MMX, SSE, SSE2, SSE3 et, SSE4) et le parallélisme externe, c'est-à-dire la façon de regrouper les processeurs entre eux.

### Sources

- [https://en.wikipedia.org/wiki/SWAR](https://en.wikipedia.org/wiki/SWAR)
- [https://fr.wikipedia.org/wiki/SWAR](https://fr.wikipedia.org/wiki/SWAR)

## Reduced instruction set computer

A reduced instruction set computer, or RISC (/rɪsk/), is a computer with a small, highly optimized set of instructions, rather than the more specialized set often found in other types of architecture, such as in a complex instruction set computer (CISC). The main distinguishing feature of RISC architecture is that the instruction set is optimized with a large number of registers and a highly regular instruction pipeline, allowing a low number of clock cycles per instruction (CPI). Core features of a RISC philosophy are a load/store architecture, in which memory is accessed through specific instructions rather than as a part of most instructions in the set, and requiring only single-cycle instructions.
Although a number of computers from the 1960s and 1970s have been identified as forerunners of RISCs, the modern concept dates to the 1980s. In particular, two projects at Stanford University and the University of California, Berkeley are most associated with the popularization of this concept. Stanford's MIPS would go on to be commercialized as the successful MIPS architecture, while Berkeley's RISC gave its name to the entire concept and was commercialized as the SPARC. Another success from this era was IBM's effort that eventually led to the IBM POWER instruction set architecture, PowerPC, and Power ISA. As these projects matured, a variety of similar designs flourished in the late 1980s and especially the early 1990s, representing a major force in the Unix workstation market as well as for embedded processors in laser printers, routers and similar products.
The many varieties of RISC designs include ARC, Alpha, Am29000, ARM, Atmel AVR, Blackfin, i860, i960, M88000, MIPS, PA-RISC, Power ISA (including PowerPC), RISC-V, SuperH, and SPARC. The use of ARM architecture processors in smartphones and tablet computers such as the iPad and Android devices provided a wide user base for RISC-based systems. RISC processors are also used in supercomputers, such as  Fugaku, which, as of June 2020, is the world's fastest supercomputer.

### Processeur à jeu d'instructions réduit

Vous pouvez améliorer sa rédaction !
Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
Un processeur à jeu d'instructions réduit (en anglais RISC pour Reduced instruction set computer) est un type d'architecture de processeur qui se caractérise par des d'instructions de base aisées à décoder, uniquement composé d'instructions simples.

### Sources

- [https://en.wikipedia.org/wiki/Reduced_instruction_set_computer](https://en.wikipedia.org/wiki/Reduced_instruction_set_computer)
- [https://fr.wikipedia.org/wiki/Processeur_%C3%A0_jeu_d%27instructions_r%C3%A9duit](https://fr.wikipedia.org/wiki/Processeur_%C3%A0_jeu_d%27instructions_r%C3%A9duit)

## Supercomputer

A supercomputer is a computer with a high level of performance as compared to a general-purpose computer. The performance of a supercomputer is commonly measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS). Since 2017, there are supercomputers which can perform over 1017 FLOPS (a hundred quadrillion FLOPS, 100 petaFLOPS or 100 PFLOPS). Since November 2017, all of the world's fastest 500 supercomputers run Linux-based operating systems. Additional research is being conducted in the United States, the European Union, Taiwan, Japan, and China to build faster, more powerful and technologically superior exascale supercomputers.
Supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields, including quantum mechanics, weather forecasting, climate research, oil and gas exploration, molecular modeling (computing the structures and properties of chemical compounds, biological macromolecules, polymers, and crystals), and physical simulations (such as simulations of the early moments of the universe, airplane and spacecraft aerodynamics, the detonation of nuclear weapons, and nuclear fusion). They have been essential in the field of cryptanalysis.
Supercomputers were introduced in the 1960s, and for several decades the fastest were made by Seymour Cray at Control Data Corporation (CDC), Cray Research and subsequent companies bearing his name or monogram. The first such machines were highly tuned conventional designs that ran faster than their more general-purpose contemporaries. Through the decade, increasing amounts of parallelism were added, with one to four processors being typical. In the 1970s, vector processors operating on large arrays of data came to dominate. A notable example is the highly successful Cray-1 of 1976. Vector computers remained the dominant design into the 1990s. From then until today, massively parallel supercomputers with tens of thousands of off-the-shelf processors became the norm.

### Superordinateur

Un superordinateur ou supercalculateur est un ordinateur conçu pour atteindre les plus hautes performances possibles avec les techniques connues lors de sa conception, en particulier en ce qui concerne la vitesse de calcul.
La science des superordinateurs est appelée « calcul haute performance » (en anglais : high-performance computing ou HPC). Cette discipline se divise en deux : la partie matérielle (conception électronique de l'outil de calcul) et la partie logicielle (adaptation logicielle du calcul à l'outil). Ces deux parties font appel à des champs de connaissances différents.
Les premiers superordinateurs (ou supercalculateurs) apparaissent dans les années 1960.
En 1961, IBM développe l'IBM Stretch ou IBM 7030, dont une unité est exploitée en France en 1963.
À cette époque, et jusque dans les années 1970, le plus important constructeur mondial de superordinateurs est la société Control Data Corporation (CDC), avec son concepteur Seymour Cray. Par la suite, Cray Research, fondée par Seymour Cray après son départ de CDC, prend l’avantage sur ses autres concurrents, jusqu’aux alentours de l'année 1990. Dans les années 1980, à l’image de ce qui s’était produit sur le marché des micro-ordinateurs des années 1970, de nombreuses petites sociétés se lancèrent sur ce marché, mais la plupart disparaissent dans le « crash » du marché des superordinateurs, au milieu des années 1990.

### Sources

- [https://en.wikipedia.org/wiki/Supercomputer](https://en.wikipedia.org/wiki/Supercomputer)
- [https://fr.wikipedia.org/wiki/Superordinateur](https://fr.wikipedia.org/wiki/Superordinateur)

## Linux

Linux (/ˈlinʊks/ (listen) LEEN-uuks or /ˈlɪnʊks/ LIN-uuks) is a family of open-source Unix-like operating systems based on the Linux kernel, an operating system kernel first released on September 17, 1991, by Linus Torvalds. Linux is typically packaged in a Linux distribution.
Distributions include the Linux kernel and supporting system software and libraries, many of which are provided by the GNU Project. Many Linux distributions use the word "Linux" in their name, but the Free Software Foundation uses the name "GNU/Linux" to emphasize the importance of GNU software, causing some controversy.

### Linux

Linux ou GNU/Linux est une famille de systèmes d'exploitation open source de type Unix fondé sur le noyau Linux, créé en 1991 par Linus Torvalds. De nombreuses distributions Linux ont depuis vu le jour et constituent un important vecteur de popularisation du mouvement du logiciel libre.
Si à l'origine, Linux a été développé pour les ordinateurs compatibles PC, il n'a jamais équipé qu'une très faible part des ordinateurs personnels. Mais le noyau Linux, accompagné ou non des logiciels GNU, est également utilisé par d'autres types de systèmes informatiques, notamment les serveurs, téléphones portables, systèmes embarqués ou encore superordinateurs. Le système d'exploitation pour téléphones portables Android qui utilise le noyau Linux mais pas GNU, équipe aujourd'hui 85 % des tablettes tactiles et smartphones.
Alors que l'usage du seul nom Linux s'est d'abord répandu pour décrire tant le noyau Linux que l'ensemble du système d'exploitation, le nom GNU/Linux a été initié par le projet Debian, et est défendu notamment par Richard Stallman, fondateur du projet GNU. Selon ses défenseurs, il est nécessaire pour créditer à la fois les développeurs de GNU et de Linux. Linux reste le plus répandu, et est notamment défendu par Linus Torvalds. Le principal argument des promoteurs de cette appellation est l'argument de simplicité : « Linux » est plus court à écrire et prononcer que « GNU/Linux ».

### Sources

- [https://en.wikipedia.org/wiki/Linux](https://en.wikipedia.org/wiki/Linux)
- [https://fr.wikipedia.org/wiki/Linux](https://fr.wikipedia.org/wiki/Linux)

## Free Software Foundation

The Free Software Foundation (FSF) is a 501(c)(3) non-profit organization founded by Richard Stallman on October 4, 1985, to support the free software movement, which promotes the universal freedom to study, distribute, create, and modify computer software, with the organization's preference for software being distributed under copyleft ("share alike") terms, such as with its own GNU General Public License. The FSF was incorporated in Boston, Massachusetts, US, where it is also based.
From its founding until the mid-1990s, FSF's funds were mostly used to employ software developers to write free software for the GNU Project. Since the mid-1990s, the FSF's employees and volunteers have mostly worked on legal and structural issues for the free software movement and the free software community.
Consistent with its goals, the FSF aims to use only free software on its own computers.
The Free Software Foundation was founded in 1985 as a non-profit corporation supporting free software development. It continued existing GNU projects such as the sale of manuals and tapes, and employed developers of the free software system. Since then, it has continued these activities, as well as advocating for the free software movement. The FSF is also the steward of several free software licenses, meaning it publishes them and has the ability to make revisions as needed.

### Free Software Foundation

modifier - modifier le code - modifier WikidataLa Free Software Foundation (FSF) (littéralement « Fondation pour le logiciel libre »), est une organisation américaine à but non lucratif fondée par Richard Stallman le 4 octobre 1985, dont la mission est la promotion du logiciel libre et la défense des utilisateurs.
La FSF aide également au financement du projet GNU depuis l'origine. Son nom est associé au mouvement du logiciel libre.

### Sources

- [https://en.wikipedia.org/wiki/Free_Software_Foundation](https://en.wikipedia.org/wiki/Free_Software_Foundation)
- [https://fr.wikipedia.org/wiki/Free_Software_Foundation](https://fr.wikipedia.org/wiki/Free_Software_Foundation)

## Free software movement

The free software movement is a social movement with the goal of obtaining and guaranteeing certain freedoms for software users, namely the freedoms to run the software, to study the software, to modify the software, and to share copies of the software (whether modified or not). Software which meets these requirements ("The Four Essential Freedoms of Free Software") is termed free software. The word 'free' is ambiguous in English, although in this context, it means 'free as in freedom', not 'free as in zero price'. A common example is, "to think of free speech, not free beer."
Although drawing on traditions and philosophies among members of the 1970s hacker culture and academia, Richard Stallman formally founded the movement in 1983 by launching the GNU Project. Stallman later established the Free Software Foundation in 1985 to support the movement.
The philosophy of the movement is that the use of computers should not lead to people being prevented from cooperating with each other. In practice, this means rejecting proprietary software, which imposes such restrictions, and promoting free software, with the ultimate goal of liberating everyone in cyberspace – that is, every computer user. Stallman notes that this action will promote rather than hinder the progression of technology, since, "It means that much wasteful duplication of system programming effort will be avoided. This effort can go instead into advancing the state of the art."
Members of the free software movement believe that all users of software should have the freedoms listed in The Free Software Definition. Many of them hold that: it is immoral to prohibit or prevent people from exercising these freedoms; these freedoms are required to create a decent society where software users can help each other; and they are necessary to have control over their computers.

### Mouvement du logiciel libre

Vous pouvez partager vos connaissances en l’améliorant (comment ?) selon les recommandations des projets correspondants.
Le mouvement du logiciel libre est un mouvement politique et social dont la démarche vise à proposer des logiciels exempts de secret industriel, sur lesquels la libre circulation du code source est garantie.
Selon Richard Stallman, initiateur de ce mouvement, « les éditeurs de logiciel cherchent à diviser et à conquérir les utilisateurs en forçant chacun à accepter de ne pas partager avec les autres. » Le mouvement du logiciel libre doit au contraire favoriser la solidarité et la coopération entre les personnes qui utilisent des ordinateurs.
En pratique, cela implique de rejeter les logiciels propriétaires, qui imposent de telles restrictions, et de promouvoir les logiciels libres, dans le but ultime de libérer tout le monde dans le cyberespace. Stallman affirme que le mouvement du logiciel libre favorisera plutôt qu'il n'entravera la progression de la technologie, car cela permettra d'« éviter de perdre beaucoup d'énergie à faire de la programmation système en double, et qu'on pourra rediriger ces efforts vers le progrès méthodologique. »

### Sources

- [https://en.wikipedia.org/wiki/Free_software_movement](https://en.wikipedia.org/wiki/Free_software_movement)
- [https://fr.wikipedia.org/wiki/Mouvement_du_logiciel_libre](https://fr.wikipedia.org/wiki/Mouvement_du_logiciel_libre)

## GNU Project

The GNU Project (/ɡnuː/ (listen)) is a free software, mass collaboration project that Richard Stallman announced on September 27, 1983. Its goal is to give computer users freedom and control in their use of their computers and computing devices by collaboratively developing and publishing software that gives everyone the rights to freely run the software, copy and distribute it, study it, and modify it. GNU software grants these rights in its license.
In order to ensure that the entire software of a computer grants its users all freedom rights (use, share, study, modify), even the most fundamental and important part, the operating system (including all its numerous utility programs) needed to be free software. According to its manifesto, the founding goal of the project was to build a free operating system, and if possible, "everything useful that normally comes with a Unix system so that one could get along without any software that is not free." Stallman decided to call this operating system GNU (a recursive acronym meaning "GNU's not Unix!"), basing its design on that of Unix, a proprietary operating system. Development was initiated in January 1984. In 1991, the Linux kernel appeared, developed outside the GNU project by Linus Torvalds, and in December 1992 it was made available under version 2 of the GNU General Public License. Combined with the operating system utilities already developed by the GNU project, it allowed for the first operating system that was free software, commonly known as Linux.
The project's current work includes software development, awareness building, political campaigning and sharing of the new material.
Richard Stallman announced his intent to start coding the GNU Project in a Usenet message in September 1983.

### Projet GNU

modifier - modifier le code - modifier WikidataLe projet GNU est un projet informatique dont les premiers développements ont été réalisés en janvier 1984 par Richard Stallman pour développer le système d’exploitation GNU. Le projet est maintenu par une communauté de hackers organisée en sous-projets. Chaque brique du projet est un logiciel libre utilisable de par sa nature dans des projets tiers, mais dont la finalité est de s’inscrire dans une logique cohérente,, avec l’ensemble des sous-projets en vue de la réalisation d’un système d’exploitation complet et entièrement libre, et avec pour stratégie, l’utilisation de l’existant.
C’est ainsi que la première version fonctionnelle du système GNU est construite en 1992 avec l’utilisation du noyau Linux, un projet développé indépendamment du projet GNU par Linus Torvalds. Mais si la « rencontre GNU/Linux » permet l’assemblage d’un système libre, le développement d’un micro-noyau reste aujourd’hui l’un des objectifs techniques du projet.
Le projet est soutenu par la Free Software Foundation, en assurant notamment sa protection légale par la gestion de ses droits d'auteurs,. Les objectifs et la philosophie du projet sont par ailleurs définis dans le manifeste GNU, lequel représente l’acte fondateur du mouvement du logiciel libre. Le projet GNU s’inscrit enfin dans une démarche sociale en plaçant les fondements philosophiques du mouvement devant les objectifs techniques du projet,.

### Sources

- [https://en.wikipedia.org/wiki/GNU_Project](https://en.wikipedia.org/wiki/GNU_Project)
- [https://fr.wikipedia.org/wiki/Projet_GNU](https://fr.wikipedia.org/wiki/Projet_GNU)

## Operating system

An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs.
Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.
For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer –  from cellular phones and video game consoles to web servers and supercomputers.

### Système d'exploitation

En informatique, un système d'exploitation (souvent appelé OS — de l'anglais Operating System) est un ensemble de programmes qui dirige l'utilisation des ressources d'un ordinateur par des logiciels applicatifs.
Il reçoit des demandes d'utilisation des ressources de l'ordinateur — ressources de stockage des mémoires (par exemple des accès à la mémoire vive, aux disques durs), ressources de calcul du processeur central, ressources de communication vers des périphériques (pour parfois demander des ressources de calcul au GPU par exemple ou tout autre carte d'extension) ou via le réseau — de la part des logiciels applicatifs. Le système d'exploitation gère les demandes ainsi que les ressources nécessaires évitant les interférences entre les logiciels.
Le système d'exploitation est le logiciel principal d'un ordinateur car il permet aux programmes de fonctionner après que le programme d'amorçage a configuré tous les périphériques lors du démarrage de l'ordinateur.
Il offre une suite de services généraux facilitant la création de logiciels applicatifs et sert d'intermédiaire entre ces logiciels et le matériel informatique. Un système d'exploitation apporte commodité, efficacité et capacité d'évolution, permettant d'introduire de nouvelles fonctions et du nouveau matériel sans remettre en cause les logiciels.

### Sources

- [https://en.wikipedia.org/wiki/Operating_system](https://en.wikipedia.org/wiki/Operating_system)
- [https://fr.wikipedia.org/wiki/Syst%C3%A8me_d%27exploitation](https://fr.wikipedia.org/wiki/Syst%C3%A8me_d%27exploitation)

## Memory management

Memory management is a form of resource management applied to computer memory.  The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed. This is critical to any advanced computer system where more than a single process might be underway at any time.
Several methods have been devised that increase the effectiveness of memory management. Virtual memory systems separate the memory addresses used by a process from actual physical addresses, allowing separation of processes and increasing the size of the virtual address space beyond the available amount of RAM using paging or swapping to secondary storage. The quality of the virtual memory manager can have an extensive effect on overall system performance.
In some operating systems, e.g., DOS/360 and successors, OS/360 and successors, allocation of storage within an address space is handled by the operating system; in, e.g., Unix-like operating systems, allocation within an address space is at the application level.
Memory management within an address space is generally categorized as either automatic memory management, usually involving garbage collection, or manual memory management.

### Gestion de la mémoire

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
Vous pouvez partager vos connaissances en l’améliorant (comment ?) selon les recommandations des projets correspondants.
La gestion de la mémoire est une forme de gestion des ressources appliquée à la mémoire de l'ordinateur. L'exigence essentielle de la gestion de la mémoire est de fournir des moyens d'allouer dynamiquement des portions de mémoire aux programmes à leur demande, et de les libérer pour réutilisation lorsqu'elles ne sont plus nécessaires. Ceci est essentiel pour tout système informatique avancé où plus d'un processus peuvent être en cours à tout moment.

### Sources

- [https://en.wikipedia.org/wiki/Memory_management](https://en.wikipedia.org/wiki/Memory_management)
- [https://fr.wikipedia.org/wiki/Gestion_de_la_m%C3%A9moire](https://fr.wikipedia.org/wiki/Gestion_de_la_m%C3%A9moire)

## System call

In computing, a system call (commonly abbreviated to syscall) is the programmatic way in which a computer program requests a service from the kernel of the operating system on which it is executed. This may include hardware-related services (for example, accessing a hard disk drive), creation and execution of new processes, and communication with integral kernel services such as process scheduling.  System calls provide an essential interface between a process and the operating system.
In most systems, system calls can only be made from userspace processes, while in some systems, OS/360 and successors for example, privileged system code also issues system calls.
The architecture of most modern processors, with the exception of some embedded systems, involves a security model. For example, the rings model specifies multiple privilege levels under which software may be executed: a program is usually limited to its own address space so that it cannot access or modify other running programs or the operating system itself, and is usually prevented from directly manipulating hardware devices (e.g. the frame buffer or network devices).

### Appel système

En informatique, un appel système (c'est-à-dire appel au système [d'exploitation]) (en anglais, system call, abrégé en syscall) désigne le moment où un programme s'interrompt pour demander au système d'exploitation d'accomplir pour lui une certaine tâche. L'expression désigne donc aussi la fonction primitive elle-même fournie par le noyau d'un système d'exploitation. Sont ainsi contrôlées et uniformisées les applications de l'espace utilisateur ; certains droits d'accès peuvent de plus être réservés au noyau. Le noyau lui-même utilise ces fonctions, qui deviennent la seule porte entre un programme et le « monde extérieur ».
Le rôle du noyau est de gérer les ressources matérielles (il contient des pilotes de périphériques) et de fournir aux programmes une interface uniforme pour l'accès à ces ressources. Dans cette configuration, par exemple, toutes les instructions de lecture ou d'écriture sur un média externe doivent passer par le noyau, qui les passe au pilote, lequel les passe au média. L'exigence de modularité est ainsi satisfaite.
Quelques appels système classiques :
Sur la majorité des systèmes d'exploitation, les appels système peuvent être utilisés comme de simples fonctions écrites en C.

### Sources

- [https://en.wikipedia.org/wiki/System_call](https://en.wikipedia.org/wiki/System_call)
- [https://fr.wikipedia.org/wiki/Appel_syst%C3%A8me](https://fr.wikipedia.org/wiki/Appel_syst%C3%A8me)

## Web server

A web server is computer software and underlying hardware that accepts requests via HTTP, the network protocol created to distribute web pages, or its secure variant HTTPS. A user agent, commonly a web browser or web crawler, initiates communication by making a request for a specific resource using HTTP, and the server responds with the content of that resource or an error message. The server can also accept and store resources sent from the user agent if configured to do so.
A server can be a single computer, or even an embedded system such as a router with a built-in configuration interface, but high-traffic websites typically run web servers on fleets of computers designed to handle large numbers of requests for documents, multimedia files and interactive scripts. A resource sent from a web server can be a preexisting file available to the server, or it can be generated at the time of the request by another program that communicates with the server program. The former is often faster and more easily cached for repeated requests, while the latter supports a broader range of applications. Websites that serve generated content usually incorporate stored files whenever possible.
Technologies such as REST and SOAP, which use HTTP as a basis for general computer-to-computer communication, have extended the application of web servers well beyond their original purpose of serving human-readable pages.

### Serveur web

Un serveur web est, soit un logiciel de service de ressources web (serveur HTTP), soit un serveur informatique (ordinateur) qui répond à des requêtes du World Wide Web sur un réseau public (Internet) ou privé (intranet),,, en utilisant principalement le protocole HTTP.
Un serveur informatique peut être utilisé à la fois pour servir des ressources du Web et pour faire fonctionner en parallèle d'autres services liés comme l'envoi d'e-mails, l'émission de flux streaming, le stockage de données via des bases de données, le transfert de fichiers par FTP, etc.
Les serveurs web publics sont reliés à Internet et hébergent des ressources (pages web, images, vidéos, etc.) du Web. Ces ressources peuvent être statiques (servies telle-quelles) ou dynamiques (construites à la demande par le serveur).
Certains serveurs sont seulement accessibles sur des réseaux privés (intranets) et hébergent des sites utilisateurs, des documents, ou des logiciels, internes à une entreprise, une administration, etc.

### Sources

- [https://en.wikipedia.org/wiki/Web_server](https://en.wikipedia.org/wiki/Web_server)
- [https://fr.wikipedia.org/wiki/Serveur_web](https://fr.wikipedia.org/wiki/Serveur_web)

## User agent

In computing, a user agent is any software, acting on behalf of a user, which "retrieves, renders and facilitates end-user interaction with Web content." A user agent is therefore a special kind of software agent.
Some prominent examples of user agents are web browsers and email readers. Often, a user agent acts as the client in a client–server system. In some contexts, such as within the Session Initiation Protocol (SIP), the term user agent refers to both end points of a communications session.
When a software agent operates in a network protocol, it often identifies itself, its application type, operating system, software vendor, or software revision, by submitting a characteristic identification string to its operating peer. In HTTP, SIP, and NNTP protocols, this identification is transmitted in a header field User-Agent. Bots, such as Web crawlers, often also include a URL and/or e-mail address so that the Webmaster can contact the operator of the bot.
In HTTP, the User-Agent string is often used for content negotiation, where the origin server selects suitable content or operating parameters for the response. For example, the User-Agent string might be used by a web server to choose variants based on the known capabilities of a particular version of client software. The concept of content tailoring is built into the HTTP standard in RFC 1945 "for the sake of tailoring responses to avoid particular user agent limitations.”

### User agent

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
La mise en forme du texte ne suit pas les recommandations de Wikipédia : il faut le « wikifier ».

### Sources

- [https://en.wikipedia.org/wiki/User_agent](https://en.wikipedia.org/wiki/User_agent)
- [https://fr.wikipedia.org/wiki/User_agent](https://fr.wikipedia.org/wiki/User_agent)

## Client–server model

Client–server model is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers, and service requesters, called clients. Often clients and servers communicate over a computer network on separate hardware, but both client and server may reside in the same system. A server host  runs one or more server programs, which share their resources with clients. A client usually does not share any of its resources, but it requests content or service from a server. Clients, therefore, initiate communication sessions with servers, which await incoming requests.
Examples of computer applications that use the client–server model are email, network printing, and the World Wide Web.
The "client–server" characteristic describes the relationship of cooperating programs in an application. The server component provides a function or service to one or many clients, which initiate requests for such services.
Servers are classified by the services they provide. For example, a web server serves web pages and a file server serves computer files. A shared resource may be any of the server computer's software and electronic components, from programs and data to processors and storage devices. The sharing of resources of a server constitutes a service.
Whether a computer is a client, a server, or both, is determined by the nature of the application that requires the service functions. For example, a single computer can run web server and file server software at the same time to serve different data to clients making different kinds of requests. Client software can also communicate with server software within the same computer. Communication between servers, such as to synchronize data, is sometimes called inter-server or server-to-server communication.
In general, a service is an abstraction of computer resources and a client does not have to be concerned with how the server performs while fulfilling the request and delivering the response. The client only has to understand the response based on the well-known application protocol, i.e. the content and the formatting of the data for the requested service.

### Client-serveur

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
Le protocole ou environnement client–serveur désigne un mode de transaction (souvent à travers un réseau) entre plusieurs programmes ou processus : l'un, qualifié de client, envoie des requêtes ; l'autre, qualifié de serveur, attend les requêtes des clients et y répond. Le serveur offre ici un service au client. Par extension, le client désigne souvent l'ordinateur sur lequel est exécuté le logiciel client, et le serveur, l'ordinateur sur lequel est exécuté le logiciel serveur. Les machines serveurs sont généralement dotées de capacités supérieures à celles des ordinateurs personnels en ce qui concerne la puissance de calcul, les entrées-sorties et les connexions réseau, afin de pouvoir répondre de manière efficace à un grand nombre de clients. Les clients sont souvent des ordinateurs personnels ou terminaux individuels (téléphone, tablette), mais pas systématiquement. Un serveur peut répondre aux requêtes de plusieurs clients. Parfois le client et le serveur peuvent être sur la même machine.

### Sources

- [https://en.wikipedia.org/wiki/Client%E2%80%93server_model](https://en.wikipedia.org/wiki/Client%E2%80%93server_model)
- [https://fr.wikipedia.org/wiki/Client-serveur](https://fr.wikipedia.org/wiki/Client-serveur)

## Interrupt

In digital computers, an interrupt is a response by the processor to an event that needs attention from the software. An interrupt condition alerts the processor and serves as a request for the processor to interrupt the currently executing code when permitted, so that the event can be processed in a timely manner. If the request is accepted, the processor responds by suspending its current activities, saving its state, and executing a function called an interrupt handler (or an interrupt service routine, ISR) to deal with the event. This interruption is temporary, and, unless the interrupt indicates a fatal error, the processor resumes normal activities after the interrupt handler finishes.
Interrupts are commonly used by hardware devices to indicate electronic or physical state changes that require attention. Interrupts are also commonly used to implement computer multitasking, especially in real-time computing. Systems that use interrupts in these ways are said to be interrupt-driven.
Interrupt signals may be issued in response to hardware or software events. These are classified as hardware interrupts or software interrupts, respectively. For any particular processor, the number of interrupt types is limited by the architecture.
A hardware interrupt is a condition related to the state of the hardware that may be signaled by an external hardware device, e.g., an interrupt request (IRQ) line on a PC, or detected by devices embedded in processor logic (e.g., the CPU timer in IBM System/370), to communicate that the device needs attention from the operating system (OS) or, if there is no OS, from the "bare-metal" program running on the CPU. Such external devices may be part of the computer (e.g., disk controller) or they may be external peripherals. For example, pressing a keyboard key or moving a mouse plugged into a PS/2 port triggers hardware interrupts that cause the processor to read the keystroke or mouse position.

### Interruption (informatique)

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
En informatique, une interruption est une suspension temporaire de l'exécution d'un programme informatique par le microprocesseur afin d'exécuter un programme prioritaire (appelé service d'interruption).
Dans son acception la plus stricte, le terme ne désigne que des interruptions dont l'exécution est provoquée par des causes externes au programme[réf. nécessaire] : avancement d'une horloge, signalisation de la complétion d'un transfert de données, positionnement des têtes de lecture/écriture, etc. Cependant, on l'utilise aussi pour désigner des exceptions, c'est-à-dire des arrêts provoqués par une condition exceptionnelle dans le programme (instruction erronée, accès à une zone mémoire inexistante, calcul arithmétique incorrect, appel volontaire au système d'exploitation, etc.). On parle alors parfois d'interruptions asynchrones pour désigner celles résultant d'un événement externe, et d'interruptions synchrones pour désigner les exceptions provoquées par le déroulement du programme. Ces dernières étaient nommées déroutements en terminologie CII-Honeywell-Bull[réf. nécessaire].

### Sources

- [https://en.wikipedia.org/wiki/Interrupt](https://en.wikipedia.org/wiki/Interrupt)
- [https://fr.wikipedia.org/wiki/Interruption_(informatique)](https://fr.wikipedia.org/wiki/Interruption_(informatique))

## Real-time computing

Real-time computing (RTC), or reactive computing is the computer science term for hardware and software systems subject to a "real-time constraint", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as "deadlines".
Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually guarantee a response within any timeframe, although typical or expected response times may be given. Real-time processing fails if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.
A real-time system has been described as one which "controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time". The term "real-time" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean "without significant delay".
Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems, and real-time networks, each of which provide essential frameworks on which to build a real-time software application.

### Système temps réel

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
En informatique, on parle d'un système temps réel lorsque ce système est capable de contrôler (ou piloter) un procédé physique à une vitesse adaptée à l'évolution du procédé contrôlé.
Les systèmes informatiques temps réel se différencient des autres systèmes informatiques par la prise en compte de contraintes temporelles dont le respect est aussi important que l'exactitude du résultat, autrement dit le système ne doit pas simplement délivrer des résultats exacts, il doit les délivrer dans des délais imposés.

### Sources

- [https://en.wikipedia.org/wiki/Real-time_computing](https://en.wikipedia.org/wiki/Real-time_computing)
- [https://fr.wikipedia.org/wiki/Syst%C3%A8me_temps_r%C3%A9el](https://fr.wikipedia.org/wiki/Syst%C3%A8me_temps_r%C3%A9el)

## Industrial control system

Industrial control system (ICS) is a general term that encompasses several types of control systems and associated instrumentation used for industrial process control.
Such systems can range in size from a few modular panel-mounted controllers to large interconnected and interactive distributed control systems with many thousands of field connections. Systems receive data from remote sensors measuring process variables (PVs), compare the collected data with desired setpoints (SPs), and derive command functions which are used to control a process through the final control elements (FCEs), such as control valves.
Larger systems are usually implemented by supervisory control and data acquisition (SCADA) systems, or distributed control systems (DCS), and programmable logic controllers (PLCs), though SCADA and PLC systems are scalable down to small systems with few control loops. Such systems are extensively used in industries such as chemical processing, pulp and paper manufacture, power generation, oil and gas processing, and telecommunications.

### Supervision (informatique)

Vous pouvez partager vos connaissances en l’améliorant (comment ?) selon les recommandations des projets correspondants.
La supervision est une technique industrielle de suivi et de pilotage informatique de procédés de fabrication automatisés. La supervision concerne l'acquisition de données (mesures, alarmes, retour d'état de fonctionnement) et des paramètres de commande des processus généralement confiés à des automates programmables.
Dans l'informatique, la supervision est la surveillance du bon fonctionnement d’un système ou d’une activité.
À ne pas confondre avec l'hypervision, qui elle correspond à la centralisation des outils de supervision, d’infrastructure, d'applications et de référentiels (ex. : CMDB).

### Sources

- [https://en.wikipedia.org/wiki/Industrial_control_system](https://en.wikipedia.org/wiki/Industrial_control_system)
- [https://fr.wikipedia.org/wiki/Supervision_(informatique)](https://fr.wikipedia.org/wiki/Supervision_(informatique))

## Programmable logic controller

A programmable logic controller (PLC) or programmable controller is an industrial digital computer that has been ruggedized and adapted for the control of manufacturing processes, such as assembly lines, robotic devices, or any activity that requires high reliability, ease of programming, and process fault diagnosis. Dick Morley is considered as the father of PLC as he had invented the first PLC, the Modicon 084, for General Motors in 1968.
PLCs can range from small modular devices with tens of inputs and outputs (I/O), in a housing integral with the processor, to large rack-mounted modular devices with thousands of I/O, and which are often networked to other PLC and SCADA systems.
They can be designed for many arrangements of digital and analog I/O, extended temperature ranges, immunity to electrical noise, and resistance to vibration and impact. Programs to control machine operation are typically stored in battery-backed-up or non-volatile memory.

### Automate programmable industriel

Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références »
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
Vous pouvez partager vos connaissances en l’améliorant (comment ?) selon les recommandations des projets correspondants.
Un automate programmable industriel, ou API (en anglais programmable logic controller, PLC), est un dispositif électronique numérique programmable destiné à la commande de processus industriels par un traitement séquentiel. Il envoie des ordres vers les préactionneurs (partie opérative ou PO côté actionneur) à partir de données d’entrées (capteurs) (partie commande ou PC côté capteur), de consignes et d’un programme informatique.

### Sources

- [https://en.wikipedia.org/wiki/Programmable_logic_controller](https://en.wikipedia.org/wiki/Programmable_logic_controller)
- [https://fr.wikipedia.org/wiki/Automate_programmable_industriel](https://fr.wikipedia.org/wiki/Automate_programmable_industriel)

## Non-volatile memory

Non-volatile memory (NVM) or non-volatile storage is a type of computer memory that can retain stored information even after power is removed. In contrast, volatile memory needs constant power in order to retain data. Examples of non-volatile memory include flash memory, read-only memory (ROM), ferroelectric RAM, most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks, and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.
Non-volatile memory typically refers to storage in semiconductor memory chips, which store data in floating-gate memory cells consisting of floating-gate MOSFETs (metal–oxide–semiconductor field-effect transistors), including flash memory storage such as NAND flash and solid-state drives (SSD), and ROM chips such as EPROM (erasable programmable ROM) and EEPROM (electrically erasable programmable ROM). It can also be classified as traditional non-volatile disk storage.
Non-volatile memory is typically used for the task of secondary storage or long-term persistent storage. The most widely used form of primary storage today is a volatile form of random access memory (RAM), meaning that when the computer is shut down, anything contained in RAM is lost. However, most forms of non-volatile memory have limitations that make them unsuitable for use as primary storage. Typically, non-volatile memory costs more, provides lower performance, or has a limited lifetime compared to volatile random access memory.

### Mémoire non volatile

Une mémoire non volatile est une mémoire informatique qui conserve ses données en l'absence d'alimentation électrique.
On distingue plusieurs types de mémoires non volatiles :
D'un certain de point de vue très inférieures en durabilité aux supports de pierre (gravure et lithogravure), de parchemin ou de papier (écriture) ou même la pellicule (microphotographie ou photographie), les mémoires informatiques (disque dur, CD, clef USB, disquettes ou bandes magnétiques...) ne peuvent être garanties que pour quelques décennies, et souvent dans des milieux protégés et spécifiques. Le taux de remplacement ou turn-over ne peut souvent être assuré au-delà d'un facteur 100 pour les technologies les plus rapides, d'autant plus méconnues qu'elles sont récentes.
Les possibilités de stockage sur des supports minéraux (quartz minéral...) ou dans des brin d'ADN reconstitué artificiellement paraissent d'emblée plus pérennes, mais ce sont des outils encore au stade de la recherche appliquée.

### Sources

- [https://en.wikipedia.org/wiki/Non-volatile_memory](https://en.wikipedia.org/wiki/Non-volatile_memory)
- [https://fr.wikipedia.org/wiki/M%C3%A9moire_non_volatile](https://fr.wikipedia.org/wiki/M%C3%A9moire_non_volatile)

## Graphical user interface

The graphical user interface (GUI /dʒiːjuːˈaɪ/ jee-you-eye or /ˈɡuːi/) is a form of user interface that allows users to interact with electronic devices through graphical icons and audio indicator such as primary notation, instead of text-based user interfaces, typed command labels or text navigation. GUIs were introduced in reaction to the perceived steep learning curve of command-line interfaces (CLIs), which require commands to be typed on a computer keyboard.
The actions in a GUI are usually performed through direct manipulation of the graphical elements. Beyond computers, GUIs are used in many handheld mobile devices such as MP3 players, portable media players, gaming devices, smartphones and smaller household, office and industrial controls. The term GUI tends not to be applied to other lower-display resolution types of interfaces, such as video games (where head-up display (HUD) is preferred), or not including flat screens, like volumetric displays because the term is restricted to the scope of two-dimensional display screens able to describe generic information, in the tradition of the computer science research at the Xerox Palo Alto Research Center.
Designing the visual composition and temporal behavior of a GUI is an important part of software application programming in the area of human–computer interaction. Its goal is to enhance the efficiency and ease of use for the underlying logical design of a stored program, a design discipline named usability. Methods of user-centered design are used to ensure that the visual language introduced in the design is well-tailored to the tasks.
The visible graphical interface features of an application are sometimes referred to as chrome or GUI (pronounced gooey). Typically, users interact with information by manipulating visual widgets that allow for interactions appropriate to the kind of data they hold. The widgets of a well-designed interface are selected to support the actions necessary to achieve the goals of users. A model–view–controller allows flexible structures in which the interface is independent of and indirectly linked to application functions, so the GUI can be customized easily. This allows users to select or design a different skin at will, and eases the designer's work to change the interface as user needs evolve. Good user interface design relates to users more, and to system architecture less.
Large widgets, such as windows, usually provide a frame or container for the main presentation content such as a web page, email message, or drawing. Smaller ones usually act as a user-input tool.

### Interface graphique

En informatique, une interface graphique (en anglais GUI pour graphical user interface) ou un environnement graphique est un dispositif de dialogue homme-machine, dans lequel les objets à manipuler sont dessinés sous forme de pictogrammes à l'écran, de sorte que l'usager peut utiliser en imitant la manipulation physique de ces objets avec un dispositif de pointage, le plus souvent une souris.
Ce type d'interface a été créé en 1973 sur le Xerox Alto par les ingénieurs du Xerox PARC pour remplacer les interfaces en ligne de commande. Mis sur le marché à la fin des années 1970 avec le Star de Xerox, le Lisa d'Apple et popularisé par cette dernière firme avec l'ordinateur Macintosh, commercialisé en 1984,.
Les interfaces graphiques sont mises en œuvre par un ensemble de logiciels souvent inclus dans les systèmes d'exploitation (Windows) ou fournis avec eux par les distributions (Linux). Ils sont devenus vers le milieu des années 1990 le standard des appareils informatiques, notamment ordinateurs, tablettes, téléphones, récepteurs GPS et guichets automatiques de billetterie (dont bancaires).
En 1970, les ordinateurs se manipulent en tapant au clavier, voire sur carte perforée, des ordres indiquant les opérations et les noms des objets sur lesquels opérer — c'est l'interface en ligne de commande. Cette interface spartiate est imposée alors par le coût élevé des mémoires et la faible puissance des processeurs. En conséquence, « les usagers de nouveaux ordinateurs [étaient] souvent frustrés et déçus par de lourdes procédures de manipulation, des messages d'erreurs obscurs, des systèmes intolérants et confus au comportement incompréhensible, mystérieux et intimidant ». Or les prix des processeurs comme des mémoires diminuent peu à peu (voir Loi de Moore) et permettent d'envisager de transférer sur les machines une partie du travail des utilisateurs.

### Sources

- [https://en.wikipedia.org/wiki/Graphical_user_interface](https://en.wikipedia.org/wiki/Graphical_user_interface)
- [https://fr.wikipedia.org/wiki/Interface_graphique](https://fr.wikipedia.org/wiki/Interface_graphique)

## Command-line interface

A command-line interface (CLI) processes commands to a computer program in the form of lines of text. The program which handles the interface is called a command-line interpreter or command-line processor. Operating systems implement a command-line interface in a shell for interactive access to operating system functions or services. Such access was primarily provided to users by computer terminals starting in the mid-1960s, and continued to be used throughout the 1970s and 1980s on VAX/VMS, Unix systems and personal computer systems including DOS, CP/M and Apple DOS.
Today, many users rely upon graphical user interfaces and menu-driven interactions. However, some programming and maintenance tasks may not have a graphical user interface and may still use a command line.
Alternatives to the command line interface include text-based user interface menus (for example, IBM AIX SMIT), keyboard shortcuts, and various desktop metaphors centered on the pointer (usually controlled with a mouse). Examples of this include the Microsoft Windows, DOS Shell, and Mouse Systems PowerPanel. Command-line interfaces are often implemented in terminal devices that are also capable of screen-oriented text-based user interfaces that use cursor addressing to place symbols on a display screen.

### Interface en ligne de commande

Une interface en ligne de commande ou ILC (en anglais command line interface, couramment abrégé CLI) est une interface homme-machine dans laquelle la communication entre l'utilisateur et l'ordinateur s'effectue en mode texte :
Une interface en ligne de commandes peut servir aussi bien pour lancer l'exécution de divers logiciels au moyen d'un interpréteur de commandes, que pour les dialogues avec l'utilisateur de ces logiciels. C'est l'interaction fondamentale entre un homme et un ordinateur (ou tout autre équipement informatique).
Lorsqu'une interface est prête à recevoir une commande, elle l'indique par une invite de commande. Celle-ci, parfois désignée par l'anglicisme prompt, consiste en quelques caractères, en début de ligne (généralement, le nom de compte de l'utilisateur, et/ou l'unité logique par défaut, et/ou le chemin par défaut, et/ou date…), se terminant par un caractère bien connu (souvent « ] », « # », « $ » ou « > »), invitant l'utilisateur à taper une commande.
L'interface en ligne de commande est la plus ancienne des interfaces conversationnelles développées sur des ordinateurs. Avant cela, les ordinateurs fonctionnaient en traitement par lots : on soumettait à l'ordinateur des données enregistrées sur une série de cartes perforées ou une bande perforée. Ces données indiquaient à l'ordinateur quels programmes lancer et de quelles informations ces programmes disposaient pour s'exécuter. Le résultat du traitement (réussi ou erroné) était imprimé sans qu'aucun dialogue avec l'utilisateur ne soit intervenu.

### Sources

- [https://en.wikipedia.org/wiki/Command-line_interface](https://en.wikipedia.org/wiki/Command-line_interface)
- [https://fr.wikipedia.org/wiki/Interface_en_ligne_de_commande](https://fr.wikipedia.org/wiki/Interface_en_ligne_de_commande)

## Shell (computing)

In computing, a shell is a computer program which exposes an operating system's services to a human user or other program. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer's role and particular operation.  It is named a shell because it is the outermost layer around the operating system.
Command-line shells require the user to be familiar with commands and their calling syntax, and to understand concepts about the shell-specific scripting language (for example, bash).
Graphical shells place a low burden on beginning computer users, and are characterized as being easy to use. Since they also come with certain disadvantages, most GUI-enabled operating systems also provide CLI shells.
Operating systems provide various services to their users, including file management, process management (running and terminating applications), batch processing, and operating system monitoring and configuration.

### Interface système

Une interface système ou coque logicielle (shell en anglais) est une couche logicielle qui fournit l'interface utilisateur d'un système d'exploitation. Il correspond à la couche la plus externe de ce dernier. L'interface système est utilisée comme diminutif de l'interface utilisateur du système d'exploitation.
Le terme anglais « shell » vient à l'origine de la terminologie employée avec les premiers systèmes d'exploitation de type Unix où il avait le sens plus spécifique de shell Unix. Cette appellation est une métaphore (on peut la traduire par coque en français) pour désigner la couche la plus haute des interfaces des systèmes Unix, par opposition à la couche de bas niveau, appelée noyau.
Les premiers concepteurs informatiques américains avaient l’habitude de décrire les différentes couches logicielles d’un système par une analogie : celle du noyau et de la coque de noix.
En anglais, le mot « kernel » désigne le cerneau, la partie comestible du noyau de la noix. Il s’agit de la partie renfermée dans la coque. La coque étant la partie non comestible du fruit, appelée « shell » en anglais. L’idée sous-jacente étant que « pour accéder à la partie comestible (utile), il faut passer par la coque ».

### Sources

- [https://en.wikipedia.org/wiki/Shell_(computing)](https://en.wikipedia.org/wiki/Shell_(computing))
- [https://fr.wikipedia.org/wiki/Interface_syst%C3%A8me](https://fr.wikipedia.org/wiki/Interface_syst%C3%A8me)

## String (computer science)

In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures.
Depending on the programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements.
When a string appears literally in source code, it is known as a string literal or an anonymous string.
In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.

### Chaîne de caractères

En informatique, une chaîne de caractères est à la fois conceptuellement une suite ordonnée de caractères et physiquement une suite ordonnée d' unités de code (code unit). La chaîne de caractères est un type de donnée dans de nombreux langages informatiques. La traduction en anglais est string.
À l'époque des pionniers, on a communément confondu chaîne de caractères et chaîne d'octets, ce qui prête aujourd'hui à confusion, lorsque l'on ne veut pas se limiter à 255 caractères. Par extension, on parle de chaîne binaire pour décrire une séquence d'octets.
Certains langages préfèrent gérer les chaînes de caractères à partir d'unités de 16 bits.
En Unicode, le type de donnée « Unicode string »  est une séquence ordonnée de « code units ».

### Sources

- [https://en.wikipedia.org/wiki/String_(computer_science)](https://en.wikipedia.org/wiki/String_(computer_science))
- [https://fr.wikipedia.org/wiki/Cha%C3%AEne_de_caract%C3%A8res](https://fr.wikipedia.org/wiki/Cha%C3%AEne_de_caract%C3%A8res)

## POSIX

The Portable Operating System Interface (POSIX) is a family of standards specified by the IEEE Computer Society for maintaining compatibility between operating systems. POSIX defines the application programming interface (API), along with command line shells and utility interfaces, for software compatibility with variants of Unix and other operating systems.
Originally, the name "POSIX" referred to IEEE Std 1003.1-1988, released in 1988. The family of POSIX standards is formally designated as IEEE 1003 and the ISO/IEC standard number is ISO/IEC 9945.
The standards emerged from a project that began around 1985. Richard Stallman suggested the name POSIX to the IEEE instead of former IEEE-IX. The committee found it more easily pronounceable and memorable, and thus adopted it.

### POSIX

POSIX est une famille de normes techniques définie depuis 1988 par l'Institute of Electrical and Electronics Engineers (IEEE), et formellement désignée par IEEE 1003. Ces normes ont émergé d'un projet de standardisation des interfaces de programmation des logiciels destinés à fonctionner sur les variantes du système d'exploitation UNIX.
Le terme POSIX a été suggéré par Richard Stallman, qui faisait partie du comité qui écrivit la première version de la norme. L'IEEE choisit de le retenir car il était facilement mémorisable,. Les quatre premières lettres forment l’acronyme de Portable Operating System Interface (interface portable de système d'exploitation), et le X exprime l'héritage UNIX.
POSIX spécifie, dans dix-sept documents différents, les interfaces utilisateurs et les interfaces logicielles. La ligne de commande standard et l'interface de script qu'est le Bourne shell. Les autres commandes, services et utilitaires comprennent awk, echo, ed, et des centaines d'autres. Les services d'entrées/sorties de base (fichiers, terminaux, réseau) doivent être présents ; le système doit supporter certains attributs spécifiques pour les fichiers. POSIX définit aussi une interface de programmation standard, et celle-ci est prise en charge par la plupart des systèmes d'exploitation récents.
Une suite de tests pour POSIX accompagne le standard. Il est appelé PCTS (POSIX Conformance Test Suite, suite de tests pour la conformité POSIX).

### Sources

- [https://en.wikipedia.org/wiki/POSIX](https://en.wikipedia.org/wiki/POSIX)
- [https://fr.wikipedia.org/wiki/POSIX](https://fr.wikipedia.org/wiki/POSIX)

## Regular expression

A regular expression (shortened as regex or regexp; also referred to as rational expression) is a sequence of characters that specifies a search pattern. Usually such patterns are used by string-searching algorithms for "find" or "find and replace" operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory.
The concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. The concept came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax.
Regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities either built-in or via libraries, as it has uses in many situations.
The phrase regular expressions, or regexes, is often used to mean the specific, standard textual syntax for representing patterns for matching text, as distinct from the mathematical notation described below. Each character in a regular expression (that is, each character in the string describing its pattern) is either a metacharacter, having a special meaning, or a regular character that has a literal meaning. For example, in the regex b., 'b' is a literal character that matches just 'b', while '.' is a metacharacter that matches every character except a newline. Therefore, this regex matches, for example, 'b%', or 'bx', or 'b5'. Together, metacharacters and literal characters can be used to identify text of a given pattern or process a number of instances of it. Pattern matches may vary from a precise equality to a very general similarity, as controlled by the metacharacters. For example, . is a very general pattern, [a-z] (match all lower case letters from 'a' to 'z') is less general and b is a precise pattern (matches just 'b'). The metacharacter syntax is designed specifically to represent prescribed targets in a concise and flexible way to direct the automation of text processing of a variety of input data, in a form easy to type using a standard ASCII keyboard.

### Expression régulière

En informatique, une expression régulière ou expression rationnelle ou expression normale ou motif, est une chaîne de caractères, qui décrit, selon une syntaxe précise, un ensemble de chaînes de caractères possibles. Les expressions régulières sont également appelées regex (un mot-valise formé depuis l'anglais regular expression). Les expressions rationnelles sont issues des théories mathématiques des langages formels des années 1940. Leur capacité à décrire avec concision des ensembles réguliers explique qu’elles se retrouvent dans plusieurs domaines scientifiques dans les années d’après-guerre et justifie leur adoption en informatique. Les expressions régulières sont aujourd’hui utilisées pour programmer des logiciels avec des fonctionnalités de lecture, de contrôle, de modification, et d'analyse de textes ainsi que dans la manipulation des langues formelles que sont les langages informatiques.
Ces expressions régulières ont la qualité de pouvoir être décrites par des formules ou motifs, (en anglais patterns) bien plus simples que les autres moyens.
Dans les années 1940, Warren McCulloch et Walter Pitts ont décrit le système nerveux en modélisant les neurones par des automates simples. En 1956, le logicien Stephen Cole Kleene, a ensuite décrit ces modèles en termes d’ensembles réguliers et d'automates. Il est considéré comme l'inventeur des expressions régulières. En 1959, Michael O. Rabin et Dana Scott proposent le premier traitement mathématique et rigoureux de ces concepts, ce qui leur vaudra le prix Turing en 1976.
Dans ce contexte, les expressions régulières correspondent aux grammaires de type 3 (voir Grammaire formelle) de la hiérarchie de Chomsky ; elles peuvent donc être utilisées pour décrire la morphologie d’une langue.

### Sources

- [https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)
- [https://fr.wikipedia.org/wiki/Expression_r%C3%A9guli%C3%A8re](https://fr.wikipedia.org/wiki/Expression_r%C3%A9guli%C3%A8re)

## Lexical analysis

In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer, or scanner, although scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.
A lexer forms the first phase of a compiler frontend in modern processing. Analysis generally occurs in one pass.
In older languages such as ALGOL, the initial stage was instead line reconstruction, which performed unstropping and removed whitespace and comments (and had scannerless parsers, with no separate lexer). These steps are now done as part of the lexer.
Lexers and parsers are most often used for compilers, but can be used for other computer language tools, such as prettyprinters or linters. Lexing can be divided into two stages: the scanning, which segments the input string into syntactic units called lexemes and categorizes these into token classes; and the evaluating, which converts lexemes into processed values.

### Analyse lexicale

En informatique, l’analyse lexicale, lexing, segmentation ou tokenization est la conversion d’une chaîne de caractères (un texte) en une liste de symboles (tokens en anglais). Elle fait partie de la première phase de la chaîne de compilation. Ces symboles sont ensuite consommés lors de l'analyse syntaxique. Un programme réalisant une analyse lexicale est appelé un analyseur lexical, tokenizer ou lexer. Un analyseur lexical est généralement combiné à un analyseur syntaxique pour analyser la syntaxe d'un texte.
Un lexème est une suite de caractères dans un programme source qui correspond au motif d'un symbole lexical et qui est analysé par l'analyseur lexical comme une instance de ce symbole lexical.
Certains auteurs utilisent le terme « token » pour représenter à la fois la chaîne de caractère en cours de traitement par l'analyseur lexical et la structure de donnée produite en sortie de ce traitement.
Le terme « lexème » en informatique n'a pas le même sens que lexème en linguistique. En informatique sa définition se rapproche de celle de morphème en linguistique.

### Sources

- [https://en.wikipedia.org/wiki/Lexical_analysis](https://en.wikipedia.org/wiki/Lexical_analysis)
- [https://fr.wikipedia.org/wiki/Analyse_lexicale](https://fr.wikipedia.org/wiki/Analyse_lexicale)
